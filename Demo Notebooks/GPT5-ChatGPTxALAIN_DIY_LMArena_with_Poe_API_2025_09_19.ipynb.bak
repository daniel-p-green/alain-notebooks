{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6cmAPfticof"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Research Scout. Investigate everything needed to build an “LM Arena”-style chat comparison and ranking web app that runs GPT-OSS-20B as the teacher, compares 10+ Poe-hosted chat models, and surfaces telemetry for model ranking.\n",
        "\n",
        "Focus:\n",
        "- Catalogue Poe-accessible models suited for side-by-side chat evaluation (GPT-OSS-20B, GPT-OSS-120B, GPT‑5, GPT‑5-Chat, Claude 3.5 Sonnet, Claude Opus 4.1, Gemini 2.5 Pro, Grok 3, Mixtral 8x22B, Llama 3.1 405B, others you find).\n",
        "- Detail Poe API capabilities, auth, rate limits, and quirks when used via the OpenAI SDK (base_url=https://api.poe.com/v1).\n",
        "- Study LM Arena / chat-arena style UX patterns: comparison layout, ranking workflows, voting schemes, telemetry capture.\n",
        "- Identify datasets or evaluation protocols for qualitative chat comparison (pairwise Elo, Bradley-Terry, Zeno feedback, etc.).\n",
        "- Surface best practices for streaming UI, SSE handling, telemetry (latency, token usage, failure reasons), and ethics (bias disclosure, safety).\n",
        "- Include legal/licensing considerations for hosting third-party Poe models.\n",
        "\n",
        "Deliverable:\n",
        "Write a structured briefing with these sections:\n",
        "\n",
        "1. Model Catalog\n",
        "   - Table of candidate Poe models (name, provider, strengths, weaknesses, context window, streaming support, special handling).\n",
        "2. Poe API Deep Dive\n",
        "   - Auth flow, headers, tooling, rate limits, error codes, streaming behaviour, cost.\n",
        "3. LM-Arena Pattern Analysis\n",
        "   - UX & workflow takeaways from existing comparison/ranking tools, recommended voting mechanics, guardrails.\n",
        "4. Implementation References\n",
        "   - Code samples, repos, blogs, papers (at least 5 high-quality sources) relevant to multi-model chat comparison.\n",
        "5. Risks & Mitigations\n",
        "   - Latency variance, model bias, moderation, fairness, API instability.\n",
        "6. Quick-Start Checklist\n",
        "   - Environment prereqs, required env vars (.env.local with POE_API_KEY, NEXT_PUBLIC_POE_DEFAULT_MODEL, etc.), npm packages, validator tools.\n",
        "\n",
        "Quality:\n",
        "- Cross-verify model details with ≥3 sources when possible.\n",
        "- Flag any unknowns.\n",
        "- Cite URLs inline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9I-3rlxiitx"
      },
      "source": [
        "ALAIN‑Kit Research Scout Briefing\n",
        "\n",
        "Roles: Platform Architect, Evaluation Scientist, Product Counsel\n",
        "\n",
        "Objective: Build an LM Arena‑style web app that runs GPT‑OSS‑20B as the teacher, compares 10+ Poe‑hosted models, and surfaces telemetry for ranking and analysis.\n",
        "\n",
        "⸻\n",
        "\n",
        "1) Model Catalog\n",
        "\n",
        "Scope: Poe‑accessible models suitable for side‑by‑side chat evaluation. Verify per‑model details via the linked sources. Context windows on Poe can differ from vendor APIs. Treat values as indicative and log actual usage from API.\n",
        "\n",
        "Model (Poe bot name)\tProvider\tStrengths\tWeaknesses\tContext window\tStreaming\tSpecial handling\n",
        "GPT‑5 (GPT-5, gpt-5-chat-latest)\tOpenAI\tStrong coding and tool use; lower hallucinations; 400k ctx\tSometimes conservative with tool calls\t400k input, up to 128k output\tYes\tSupports reasoning effort flags on Poe page; verify tool schema fidelity. Sources: https://poe.com/GPT-5, https://openai.com/gpt-5/\n",
        "GPT‑5‑mini\tOpenAI\tFast, cheap; good for baseline\tLower reasoning depth\t400k\tYes\tUse as speed baseline. Source: https://poe.com/explore\n",
        "Claude Sonnet 4 (Claude-Sonnet-4)\tAnthropic\tHigh code quality; low hallucination; large ctx\tCan self‑limit on safety\t200k on Poe page; 1M via Anthropic enterprise\tYes\tPoe supports “thinking budget” flag; treat as text‑only for reliability. Sources: https://poe.com/Claude-Sonnet-4, https://www.anthropic.com/news/1m-context\n",
        "Claude Opus 4.1\tAnthropic\tBest human‑level reasoning among Claude family\tSlower; costly\t200k typical\tYes\tPrefer Sonnet for speed; Opus for judge cross‑checks. Source: https://www.anthropic.com/claude/sonnet\n",
        "Gemini 2.5 Pro (Gemini-2.5-Pro)\tGoogle\tMultimodal; long‑context handling\tOccasionally verbose; tool JSON drift\tUp to 1M on vendor claims\tYes\tDisable media streaming; treat as text for stability. Sources: https://poe.com/Gemini-2.5-Pro-Exp\n",
        "Grok 4 (Grok-4)\txAI\tStrong long‑context; live search capability\tStyle may be edgy unless steered\t256k\tYes\tEnsure search disabled for fair head‑to‑head unless wanted. Sources: https://docs.x.ai/docs/models/grok-4-0709, https://poe.com/xai\n",
        "Llama 3.1 405B (Llama-3.1-405B)\tMeta\tOpen model; solid coding; 128k ctx typical\tSlower via hosted endpoints\t128k vendor guidance\tYes\tGood for open‑weight parity checks. Sources: https://ai.meta.com/blog/meta-llama-3-1/, https://huggingface.co/meta-llama/Llama-3.1-405B\n",
        "Mixtral 8x22B\tMistral\tEfficient MoE; good generalist\tWeaker on chain‑of‑thought\t64k to 128k typical\tYes\tUse as classic open‑weights baseline. Source: Poe Explore and vendor docs\n",
        "Grok 3\txAI\tCheaper baseline for Grok family\tLower reasoning depth\t131k\tYes\tUse for ablations vs Grok 4. Source: xAI docs\n",
        "GPT‑OSS‑20B\tOpenAI (open‑weights)\tFast local‑class model; good judge for simple rubric; 16 GB footprint MXFP4\tMay miss subtle safety or style\t~32k effective typical; vendor evals in MXFP4\tYes\tUse as “teacher” to seed judgments and rubrics. Sources: https://huggingface.co/openai/gpt-oss-20b, https://openai.com/index/introducing-gpt-oss/\n",
        "GPT‑OSS‑120B\tOpenAI (open‑weights)\tNear o4‑mini on benchmarks; stronger judge\tHeavier; higher latency\t64k to 128k typical\tYes\tUse sparingly for gold‑standard review. Sources: https://openai.com/index/introducing-gpt-oss/\n",
        "\n",
        "Notes\n",
        "\t•\tModel availability on Poe API is tied to public bots. Private bots are not available via the OpenAI‑compatible endpoint. Source: https://creator.poe.com/docs/external-applications/openai-compatible-api\n",
        "\t•\tAlways log usage.prompt_tokens and usage.completion_tokens from Poe responses and compute normalized scores per 1k output tokens to avoid bias toward verbose models.\n",
        "\n",
        "⸻\n",
        "\n",
        "2) Poe API Deep Dive\n",
        "\n",
        "Endpoint & Auth\n",
        "\t•\tBase URL: https://api.poe.com/v1 with OpenAI SDKs by setting baseURL.\n",
        "\t•\tAuth: Authorization: Bearer $POE_API_KEY. Key from https://poe.com/api_key\n",
        "\t•\tQuickstart and streaming examples: https://creator.poe.com/docs/external-applications/openai-compatible-api\n",
        "\n",
        "Supported request fields\n",
        "\t•\tmodel, messages, max_tokens, stream, tools, tool_choice, parallel_tool_calls, temperature, stop, top_p supported. n must be 1. Many OpenAI extras are ignored. See detailed table: https://creator.poe.com/docs/external-applications/openai-compatible-api\n",
        "\n",
        "Responses\n",
        "\t•\tOpenAI‑shaped JSON. choices length 1. usage fields populated; some details arrays empty by design.\n",
        "\n",
        "Errors & Retries\n",
        "\t•\tError schema mirrors OpenAI. Common: 400 invalid_request, 401 auth, 402 insufficient_credits, 403 moderation, 404 not_found, 408 timeout, 413 too_large, 429 rate_limit, 502 upstream, 529 overloaded. Respect Retry‑After and use exponential backoff with jitter. Source: docs page above.\n",
        "\n",
        "Rate Limits & Headers\n",
        "\t•\tPublished high‑water guidance: ~500 rpm for the compatibility endpoint. Poe does not emit OpenAI rate‑limit headers; you get openai-processing-ms and x-request-id. Source: docs page above.\n",
        "\t•\tImplement client‑side leaky‑bucket with concurrency caps per model to avoid bursts. See general RL patterns: https://cookbook.openai.com/examples/how_to_handle_rate_limits and https://www.moesif.com/blog/technical/rate-limiting/Best-Practices-for-API-Rate-Limits-and-Quotas-With-Moesif-to-Avoid-Angry-Customers/\n",
        "\n",
        "Streaming quirks\n",
        "\t•\tSSE stream uses OpenAI semantics with [DONE] terminator. Some third‑party clients mis‑handle the final event. Guard with tolerant parsers. References: https://platform.openai.com/docs/api-reference/streaming, https://github.com/cline/cline/issues/1786\n",
        "\t•\tFor media bots, Poe recommends stream: false for reliability. Source: docs page.\n",
        "\n",
        "Cost\n",
        "\t•\tPoe subscription points apply to API usage. Add‑on points priced to approximate direct vendor rates. Source: https://creator.poe.com/docs/external-applications/openai-compatible-api and https://help.poe.com/hc/en-us/articles/19945140063636-Poe-Subscriptions-FAQs\n",
        "\n",
        "Model selection\n",
        "\t•\tUse bot names exactly as shown on Poe (examples: Claude-Sonnet-4, Grok-4, Gemini-2.5-Pro, Llama-3.1-405B, GPT-5). Source: docs and bot pages.\n",
        "\n",
        "⸻\n",
        "\n",
        "3) LM‑Arena Pattern Analysis\n",
        "\n",
        "References\n",
        "\t•\tChatbot Arena overview and Elo method: https://lmsys.org/blog/2023-05-03-arena/\n",
        "\t•\tChatbot Arena paper: https://arxiv.org/pdf/2403.04132\n",
        "\t•\tFastChat repo that powers Arena: https://github.com/lm-sys/FastChat\n",
        "\t•\tElo stability update: https://lmsys.org/blog/2023-12-07-leaderboard/\n",
        "\n",
        "UX takeaways\n",
        "\t•\tSide‑by‑side responses with hidden model identities to reduce brand bias.\n",
        "\t•\tOne‑click “Left wins” or “Right wins” and optional “Tie” or “Both bad”.\n",
        "\t•\tPrompt history visible on hover or modal to enable audit.\n",
        "\t•\tDebiased sampling: randomize model order and pairings; enforce temperature and top_p parity.\n",
        "\n",
        "Ranking mechanics\n",
        "\t•\tStart with Bradley‑Terry logistic preference model for pairwise outcomes; report Elo‑equivalent. Primer: https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model and paired‑eval justification: https://arxiv.org/abs/2110.10746\n",
        "\t•\tBootstrap confidence intervals as in Arena update. Aggregate across prompts stratified by domain.\n",
        "\t•\tOptionally weight judges by reliability using Crowd‑BT ideas for spam resistance.\n",
        "\n",
        "Telemetry to capture per sample\n",
        "\t•\tLatency: queue delay, first token, time to last token.\n",
        "\t•\tToken economics: prompt, completion, total; tokens per second.\n",
        "\t•\tFailure class: timeout, 4xx, 5xx, refusal, tool‑use error.\n",
        "\t•\tContent signals: refusal rate, jailbreak flags, safety incidents.\n",
        "\t•\tAnnotations: rater confidence and rationale text.\n",
        "\n",
        "Teacher‑judge workflow\n",
        "\t•\tUse GPT‑OSS‑20B to produce a rubric plus auto‑grades for each pair before human vote. Periodically spot‑check with GPT‑OSS‑120B. Keep human‑overrides as source of truth.\n",
        "\n",
        "Fairness & ethics\n",
        "\t•\tDisclose model identities after voting. Publish prompt set and sampling policy. Track refusal and bias categories. Provide rater guidance and cooling‑off for sensitive topics. See Zeno framework for behavior eval inspiration: https://dl.acm.org/doi/10.1145/3544548.3581268 and https://dig.cmu.edu/publications/2023-zeno.html\n",
        "\n",
        "⸻\n",
        "\n",
        "4) Implementation References\n",
        "\t•\tPoe OpenAI‑compatible API doc: https://creator.poe.com/docs/external-applications/openai-compatible-api\n",
        "\t•\tPoe Interface configuration: https://creator.poe.com/docs/external-applications/interface-configuration\n",
        "\t•\tLMSYS FastChat: https://github.com/lm-sys/FastChat\n",
        "\t•\tChatbot Arena paper: https://arxiv.org/pdf/2403.04132\n",
        "\t•\tArena‑Hard‑Auto benchmark: https://github.com/lmarena/arena-hard-auto\n",
        "\t•\tBradley‑Terry and paired evaluation: https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model, https://arxiv.org/abs/2110.10746\n",
        "\t•\tOpenAI streaming patterns: https://platform.openai.com/docs/api-reference/streaming\n",
        "\n",
        "Node snippet using OpenAI SDK against Poe\n",
        "\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "export const poe = new OpenAI({\n",
        "  apiKey: process.env.POE_API_KEY!,\n",
        "  baseURL: \"https://api.poe.com/v1\",\n",
        "});\n",
        "\n",
        "export async function chatOnce(model: string, messages: {role: \"system\"|\"user\"|\"assistant\"; content: string}[]) {\n",
        "  const res = await poe.chat.completions.create({ model, messages });\n",
        "  return res.choices[0]?.message?.content ?? \"\";\n",
        "}\n",
        "\n",
        "Server‑sent events streaming\n",
        "\n",
        "export async function* streamChat(model: string, messages: any[]) {\n",
        "  const stream = await poe.chat.completions.create({ model, messages, stream: true });\n",
        "  for await (const chunk of stream) {\n",
        "    yield chunk.choices?.[0]?.delta?.content ?? \"\";\n",
        "  }\n",
        "}\n",
        "\n",
        "Pairwise vote to Bradley‑Terry score\n",
        "\n",
        "// Minimal BT update: Ri, Rj are latent strengths; y in {0, 0.5, 1}\n",
        "function btUpdate(Ri: number, Rj: number, y: number, lr = 0.05) {\n",
        "  const pij = 1 / (1 + Math.exp(Rj - Ri));\n",
        "  const grad = y - pij; // logistic\n",
        "  return [Ri + lr * grad, Rj - lr * grad];\n",
        "}\n",
        "\n",
        "\n",
        "⸻\n",
        "\n",
        "5) Risks & Mitigations\n",
        "\t•\tLatency variance across providers: measure queue, network, and generation separately; show p50, p90, p99; cap generation time; pre‑warm by sending a no‑op ping.\n",
        "\t•\tVendor parameter drift: fix temperature, top_p, and max_tokens for all runs; log actual values echoed in responses.\n",
        "\t•\tModel identity bias: mask identities during voting; randomize side and order; delay reveal until after submission.\n",
        "\t•\tSafety & moderation: apply a common pre‑moderation pass; filter prohibited content; log refusals separately; offer a “Both unsafe” vote.\n",
        "\t•\tAPI instability or outages: circuit breaker per model with fallback to cached last good sample; expose degraded state in UI.\n",
        "\t•\tTool‑call incompatibility: standardize on text‑only comparisons for apples‑to‑apples; when tools are enabled, require identical tool schemas and record tool latency separately.\n",
        "\t•\tLicensing & terms: comply with Poe API Terms https://poe.com/api_terms. Only access public bots through the OpenAI‑compatible endpoint. Attribute providers when displaying results. Honor vendor usage policies and trademarks. When publishing head‑to‑head results, include disclaimers that models were accessed through Poe, may have different capabilities than direct vendor APIs, and that rates/ctx can change without notice.\n",
        "\n",
        "⸻\n",
        "\n",
        "6) Quick‑Start Checklist\n",
        "\n",
        "Environment\n",
        "\t•\tNode 20+, Next.js App Router\n",
        "\t•\t.env.local:\n",
        "\t•\tPOE_API_KEY=...\n",
        "\t•\tNEXT_PUBLIC_POE_DEFAULT_MODEL=GPT-5\n",
        "\t•\tNEXT_PUBLIC_POE_COMPARISON_MODELS=GPT-5,Claude-Sonnet-4,Grok-4,Gemini-2.5-Pro,Llama-3.1-405B,Mixtral-8x22B,GPT-OSS-20B\n",
        "\n",
        "Packages\n",
        "\t•\topenai v4 SDK\n",
        "\t•\tzod for telemetry schema\n",
        "\t•\tswr or react-query for SSE‑fed UI state\n",
        "\t•\tclsx, lucide-react, framer-motion, recharts for UI polish\n",
        "\n",
        "Validators & tooling\n",
        "\t•\tTokenize prompts server‑side and assert max prompt + expected output ≤ min(ctx) across selected models\n",
        "\t•\tSSE parser with tolerant [DONE] handling\n",
        "\t•\tRate limit guard: leaky‑bucket per model, max concurrent streams per provider\n",
        "\t•\tTelemetry hooks: first‑token time, ttfb, tte, tokens/sec, errors, refusal type\n",
        "\t•\tAudit log pipeline to a warehouse with per‑prompt stratification\n",
        "\n",
        "MVP flow\n",
        "\t1.\tUser picks two models and a prompt. App sends two parallel Poe calls with identical parameters.\n",
        "\t2.\tStream both to UI; freeze at end and show token/latency stats.\n",
        "\t3.\tUser votes: Left, Right, Tie, Both bad plus optional rationale.\n",
        "\t4.\tStore a single record with model ids, prompt hash, stats, vote, rubric auto‑grade.\n",
        "\t5.\tBatch‑update BT/Elo and render a live leaderboard with CIs.\n",
        "\n",
        "Open questions\n",
        "\t•\tExact per‑bot ctx on Poe can differ from vendor claims. Measure empirically by probing 413 request_too_large thresholds and tracking truncation behavior.\n",
        "\t•\tTool use parity: when enabling tools, which vendor features are faithfully passed through Poe today vs ignored. Track via capability probes per bot.\n",
        "\n",
        "⸻\n",
        "\n",
        "Key Sources\n",
        "\t•\tPoe OpenAI‑compatible API: https://creator.poe.com/docs/external-applications/openai-compatible-api\n",
        "\t•\tModel pages: https://poe.com/GPT-5, https://poe.com/Claude-Sonnet-4, https://poe.com/Gemini-2.5-Pro-Exp, https://poe.com/xai\n",
        "\t•\tGPT‑OSS models: https://huggingface.co/openai/gpt-oss-20b, https://openai.com/index/introducing-gpt-oss/\n",
        "\t•\tArena methodology: https://lmsys.org/blog/2023-05-03-arena/, https://arxiv.org/pdf/2403.04132, https://github.com/lm-sys/FastChat\n",
        "\t•\tPaired evaluation background: https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model, https://arxiv.org/abs/2110.10746\n",
        "\t•\tRate limit & streaming practices: https://cookbook.openai.com/examples/how_to_handle_rate_limits, https://platform.openai.com/docs/api-reference/streaming, https://github.com/cline/cline/issues/1786"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKcnguWSirFO"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Lesson Architect. Turn the research briefing into an instructional design plan for a tutorial notebook that teaches builders how to build an LM Arena-style chat comparison and ranking tool using GPT-OSS-20B (via Poe).\n",
        "\n",
        "Goals:\n",
        "- Learners: React/Next.js + TypeScript engineers, comfortable with OpenAI SDK, new to Poe specifics and ranking math.\n",
        "- Teacher: GPT-OSS-20B (Poe).\n",
        "- Deliverable: structured JSON design outlining objectives, sections, interactions, and assessments.\n",
        "\n",
        "Include:\n",
        "1. Learning Objectives\n",
        "   - 4–5 SMART objectives mapped to Bloom levels (cover setup, streaming, voting math, telemetry, ethics).\n",
        "   - Beginner / Intermediate / Advanced goal breakdown.\n",
        "2. Notebook Outline\n",
        "   - 5–7 sections with titles, estimated minutes, key concepts, planned activities, and assessment touchpoints.\n",
        "   - Ensure flow: context → setup → selector → duel UI → ranking math → telemetry + review → wrap-up.\n",
        "3. Interactive Elements\n",
        "   - Code demos, widgets (e.g., pairwise vote buttons, telemetry dashboard), exploration tasks.\n",
        "4. Assessments\n",
        "   - Formative checkpoints per section.\n",
        "   - 4 MCQs with correct answers + rationales covering Poe auth, streaming diff, ranking math, safety.\n",
        "5. Extension Ideas\n",
        "   - Optional challenges (e.g., add win-rate charts, integrate persistent storage, human preference logging).\n",
        "6. Accessibility & Cognitive Load\n",
        "   - Notes on pacing, visuals, warnings for complex math sections.\n",
        "\n",
        "Output format: valid JSON (no Markdown fences). Keys: objectives, outline, interactivity, assessments, extensions, accessibility_notes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKlVgLJjixKp"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“objectives”: {\n",
        "“beginner”: [\n",
        "“Explain how Poe’s OpenAI-compatible API differs from direct OpenAI endpoints (Bloom: Understand)”,\n",
        "“Set up environment variables and authenticate with Poe (Bloom: Apply)”\n",
        "],\n",
        "“intermediate”: [\n",
        "“Implement a streaming chat completion with GPT-OSS-20B and log latency metrics (Bloom: Apply)”,\n",
        "“Construct a pairwise voting interface and update Bradley-Terry/Elo scores (Bloom: Analyze)”\n",
        "],\n",
        "“advanced”: [\n",
        "“Design telemetry hooks that normalize across models and support fairness analysis (Bloom: Evaluate)”,\n",
        "“Integrate ethical guardrails and bias disclosure into ranking workflow (Bloom: Create)”\n",
        "]\n",
        "},\n",
        "“outline”: [\n",
        "{\n",
        "“title”: “Introduction & Context”,\n",
        "“minutes”: 8,\n",
        "“concepts”: [“Chatbot Arena history”, “Why Poe as model hub”, “Role of GPT-OSS-20B as teacher”],\n",
        "“activities”: [“Read overview”, “Discuss why pairwise evaluation beats single scores”],\n",
        "“assessment”: “Short reflection: Why hide model identities in evaluation?”\n",
        "},\n",
        "{\n",
        "“title”: “Environment Setup & Poe API”,\n",
        "“minutes”: 12,\n",
        "“concepts”: [“Auth via POE_API_KEY”, “Base URL override”, “Rate limits & error codes”],\n",
        "“activities”: [“Hands-on setup .env.local”, “Test single chat request”],\n",
        "“assessment”: “Multiple choice on correct Poe header and endpoint”\n",
        "},\n",
        "{\n",
        "“title”: “Streaming & Selector”,\n",
        "“minutes”: 15,\n",
        "“concepts”: [“SSE streaming”, “Model selector widget”, “Handling [DONE] events”],\n",
        "“activities”: [“Code demo: parallel calls to GPT-5 vs GPT-OSS-20B”, “Observe latency”],\n",
        "“assessment”: “Debug exercise: fix incomplete SSE parsing”\n",
        "},\n",
        "{\n",
        "“title”: “Duel UI & Voting Mechanics”,\n",
        "“minutes”: 20,\n",
        "“concepts”: [“Hidden model identity”, “Pairwise voting”, “Bradley-Terry update”],\n",
        "“activities”: [“Interactive widget: Left vs Right vs Tie”, “Code walkthrough of btUpdate()”],\n",
        "“assessment”: “Calculate updated scores after given votes”\n",
        "},\n",
        "{\n",
        "“title”: “Telemetry & Ethics”,\n",
        "“minutes”: 18,\n",
        "“concepts”: [“Latency metrics”, “Token cost normalization”, “Bias disclosure & safety”],\n",
        "“activities”: [“Build telemetry dashboard mockup”, “Identify unsafe content flags”],\n",
        "“assessment”: “Scenario quiz: Which telemetry signals indicate bias?”\n",
        "},\n",
        "{\n",
        "“title”: “Wrap-up & Extensions”,\n",
        "“minutes”: 7,\n",
        "“concepts”: [“Leaderboard with CI”, “Storage backends”, “Publishing results with disclaimers”],\n",
        "“activities”: [“Review extension ideas”, “Share reflections”],\n",
        "“assessment”: “Exit ticket: One risk & one mitigation”\n",
        "}\n",
        "],\n",
        "“interactivity”: {\n",
        "“code_demos”: [\n",
        "“OpenAI SDK call to Poe with model selector”,\n",
        "“Streaming generator yielding tokens”,\n",
        "“Bradley-Terry update function”\n",
        "],\n",
        "“widgets”: [\n",
        "“Model dropdown selector”,\n",
        "“Pairwise vote buttons (Left, Right, Tie, Both Bad)”,\n",
        "“Telemetry live dashboard”\n",
        "],\n",
        "“exploration_tasks”: [\n",
        "“Experiment with temperature=0.7 vs 0.2 on two models”,\n",
        "“Measure token/sec and latency for three consecutive runs”,\n",
        "“Modify Elo update to include bootstrap CI”\n",
        "]\n",
        "},\n",
        "“assessments”: {\n",
        "“formative”: [\n",
        "“Reflection questions after intro”,\n",
        "“Debug SSE parsing exercise”,\n",
        "“Math update calculation in duel section”,\n",
        "“Telemetry scenario quiz”\n",
        "],\n",
        "“mcqs”: [\n",
        "{\n",
        "“question”: “Which header is required to authenticate with Poe’s API?”,\n",
        "“options”: [“X-Auth-Key”, “Authorization: Bearer ”, “Poe-Token”, “Api-Secret”],\n",
        "“answer”: “Authorization: Bearer ”,\n",
        "“rationale”: “Poe follows OpenAI-compatible header structure; must use Authorization: Bearer.”\n",
        "},\n",
        "{\n",
        "“question”: “What differentiates Poe streaming from OpenAI streaming?”,\n",
        "“options”: [“No [DONE] token”, “Different event name”, “Identical semantics but some clients mis-handle final event”, “Binary encoding”],\n",
        "“answer”: “Identical semantics but some clients mis-handle final event”,\n",
        "“rationale”: “Poe uses same SSE spec; key quirk is tolerance for [DONE] handling.”\n",
        "},\n",
        "{\n",
        "“question”: “In the Bradley-Terry model, what does the gradient term represent?”,\n",
        "“options”: [“Refusal rate”, “Difference between observed and expected outcome”, “Token length”, “Latency variance”],\n",
        "“answer”: “Difference between observed and expected outcome”,\n",
        "“rationale”: “Bradley-Terry updates weights using y - pij (observed - expected).”\n",
        "},\n",
        "{\n",
        "“question”: “Why should models be anonymized during evaluation?”,\n",
        "“options”: [“To prevent overloading API”, “To reduce brand bias from raters”, “To save token costs”, “To simplify telemetry”],\n",
        "“answer”: “To reduce brand bias from raters”,\n",
        "“rationale”: “Concealing model identity avoids human preference bias for known providers.”\n",
        "}\n",
        "]\n",
        "},\n",
        "“extensions”: [\n",
        "“Implement persistent leaderboard with SQLite/Prisma”,\n",
        "“Add win-rate and confidence-interval charts with Recharts”,\n",
        "“Integrate human preference logging alongside GPT-OSS auto-judgments”,\n",
        "“Enable optional tool schema evaluation for advanced users”,\n",
        "“Test fairness across multilingual prompts”\n",
        "],\n",
        "“accessibility_notes”: {\n",
        "“pacing”: “Allocate ~10-15 minutes per coding section; allow pauses before math-heavy parts.”,\n",
        "“visuals”: “Use clear side-by-side UI with large buttons; colorblind-friendly palette for telemetry charts.”,\n",
        "“math”: “Warn before introducing Bradley-Terry math; provide simplified formula and numeric example first.”,\n",
        "“cognitive_load”: “Break down setup into bite-size steps; scaffold voting math with worked examples; encourage reflection after ethics.”\n",
        "}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht33S495i1Va"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Section Scribe. For each section in the design, produce balanced markdown + executable TypeScript/Next.js code cells that implement an LM Arena-style chat comparison + ranking tool on Poe with GPT-OSS-20B as highlight teacher.\n",
        "\n",
        "Instructions:\n",
        "- For each section request, emit an ordered list of notebook cells (markdown or code). Keep markdown/code ratio near 60/40.\n",
        "- Code must be ready to paste into a Next.js (App Router) project scaffolded as in Step 2 and rely on `.env.local` vars (`POE_API_KEY`, `NEXT_PUBLIC_POE_DEFAULT_MODEL`, optional `NEXT_PUBLIC_POE_COMPARISON_MODELS` comma list).\n",
        "- Use the `openai` SDK (v4) with `baseURL: https://api.poe.com/v1`.\n",
        "- Demonstrate:\n",
        "  1. Model selector (enum guard).\n",
        "  2. Pairwise “chat battle” UI (two models, dual prompts, side-by-side responses via streaming).\n",
        "  3. Voting panel capturing user preference, updating Elo-style rankings (client state acceptable).\n",
        "  4. Telemetry logging (latency, tokens, error info).\n",
        "  5. Safety callouts (warn about bias, moderation).\n",
        "- Include TypeScript types, server route handlers (`app/api/.../route.ts`), React client components, and optional utility files under `lib/`.\n",
        "- Add succinct comments only for non-obvious logic (e.g., Elo update formula).\n",
        "- For markdown narrative:\n",
        "  - Introduce concept, explain rationale, highlight gotchas (Poe rate limits, SSE chunk parsing, fairness concerns).\n",
        "  - Provide callouts (Tip/Warning/Note) where helpful.\n",
        "\n",
        "Output:\n",
        "Return JSON via `emit_section_content` with fields:\n",
        "- section_number\n",
        "- title\n",
        "- estimated_tokens\n",
        "- content: array of `{cell_type: \"markdown\"|\"code\", source: string}`\n",
        "- callouts: array of `{type, message}`\n",
        "\n",
        "No extra prose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeFhhI95i2HL"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 1,\n",
        "“title”: “Introduction, Goals, and Project Scaffold”,\n",
        "“estimated_tokens”: 1200,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 1. Why an LM Arena on Poe\\nYou will build a side-by-side chat comparison tool that streams two model responses, captures your vote, and updates rankings. We use GPT-OSS-20B on Poe as the highlight teacher to seed rubrics and provide quick judge passes. You will:\\n\\n- compare at least two Poe models per duel\\n- stream responses concurrently for fast visual feedback\\n- vote Left, Right, Tie, or Both bad\\n- update Elo-style ratings in client state\\n- log telemetry such as TTFB, total latency, and token counts\\n\\nYou will ship this in a Next.js App Router project with TypeScript and the OpenAI v4 SDK pointed at https://api.poe.com/v1.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.1 What makes Poe useful here\\n- Single OpenAI-compatible surface to many vendor models\\n- Consistent streaming semantics and usage counters\\n- Lower integration overhead than juggling multiple vendor SDKs\\n\\nYou will still validate fairness: keep identical temperature, top_p, and max_tokens; mask identities during voting; randomize sides.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.2 Minimal environment and variables\\nCreate .env.local at the repo root:\\n\\nbash\\nPOE_API_KEY=YOUR_KEY\\nNEXT_PUBLIC_POE_DEFAULT_MODEL=GPT-OSS-20B\\nNEXT_PUBLIC_POE_COMPARISON_MODELS=GPT-5,Claude-Sonnet-4,Grok-4,Gemini-2.5-Pro,Llama-3.1-405B,Mixtral-8x22B,GPT-OSS-20B\\n\\n\\nYou can change the public list later from a settings panel.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/poe.ts\\nimport OpenAI from \"openai\";\\n\\n// Single client for Poe via OpenAI SDK\\nexport const poe = new OpenAI({\\n  apiKey: process.env.POE_API_KEY!,\\n  baseURL: \"https://api.poe.com/v1\",\\n});\\n\\nexport type ChatMessage = { role: \"system\" | \"user\" | \"assistant\"; content: string };\\n\\nexport async function chatOnce(model: string, messages: ChatMessage[]) {\\n  const res = await poe.chat.completions.create({ model, messages, stream: false, max_tokens: 512 });\\n  const choice = res.choices?.[0];\\n  return {\\n    text: choice?.message?.content ?? \"\",\\n    usage: res.usage,\\n    id: res.id,\\n    model: res.model,\\n  };\\n}\\n\\n// Yields content chunks for UI streaming\\nexport async function* chatStream(model: string, messages: ChatMessage[]) {\\n  const stream = await poe.chat.completions.create({ model, messages, stream: true, max_tokens: 512 });\\n  const start = Date.now();\\n  let firstTokenAt: number | null = null;\\n  for await (const part of stream) {\\n    const delta = part.choices?.[0]?.delta?.content ?? \"\";\\n    if (delta && firstTokenAt === null) firstTokenAt = Date.now();\\n    yield { delta, ttfbMs: firstTokenAt ? firstTokenAt - start : null };\\n  }\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.3 Model catalog for the UI selector\\nYou will parse a public env list to keep the selector stable across builds. You also enforce an enum guard so API calls reject unknown names early.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/models.ts\\nconst DEFAULTS = [\\n  \"GPT-5\",\\n  \"Claude-Sonnet-4\",\\n  \"Grok-4\",\\n  \"Gemini-2.5-Pro\",\\n  \"Llama-3.1-405B\",\\n  \"Mixtral-8x22B\",\\n  \"GPT-OSS-20B\",\\n] as const;\\n\\nexport const COMPARISON_MODELS: readonly string[] = (process.env.NEXT_PUBLIC_POE_COMPARISON_MODELS?.split(\",\")\\n  .map(s => s.trim())\\n  .filter(Boolean) || DEFAULTS) as readonly string[];\\n\\nexport type PoeModel = typeof DEFAULTS[number] | (string & {});\\n\\nexport function isAllowedModel(m: string): m is PoeModel {\\n  return COMPARISON_MODELS.includes(m);\\n}\\n\\nexport const DEFAULT_MODEL: PoeModel = (process.env.NEXT_PUBLIC_POE_DEFAULT_MODEL || \"GPT-OSS-20B\") as PoeModel;”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.4 Types for telemetry and votes\\nWe standardize the record you will store per duel. This keeps later sections simple when you wire the ranking math and UI.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/types.ts\\nexport type Telemetry = {\\n  promptHash: string;\\n  leftModel: string;\\n  rightModel: string;\\n  params: { temperature: number; top_p: number; max_tokens: number };\\n  left: { ttfbMs: number | null; totalMs: number | null; promptTokens?: number; completionTokens?: number; error?: string };\\n  right: { ttfbMs: number | null; totalMs: number | null; promptTokens?: number; completionTokens?: number; error?: string };\\n};\\n\\nexport type VoteOutcome = \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\";\\n\\nexport type DuelRecord = Telemetry & {\\n  vote: VoteOutcome | null;\\n  voterNote?: string;\\n  createdAt: string; // ISO\\n};”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.5 Health routes you can ping locally\\nThe health endpoint validates env presence and enumerates allowed models. It does not make an upstream call, so you can test without burning credits.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/health/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { COMPARISON_MODELS, DEFAULT_MODEL } from \"@/lib/models\";\\n\\nexport async function GET() {\\n  const ok = Boolean(process.env.POE_API_KEY);\\n  return NextResponse.json({\\n    ok,\\n    hasKey: ok,\\n    defaultModel: DEFAULT_MODEL,\\n    models: COMPARISON_MODELS,\\n  });\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.6 Smoke test endpoint for a one-shot call\\nUse this for basic connectivity. In later sections, you will use streaming and a pairwise duel route.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/smoke/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { isAllowedModel, DEFAULT_MODEL } from \"@/lib/models\";\\n\\nexport async function POST(req: Request) {\\n  const { model = DEFAULT_MODEL, content = \"Say ‘pong’\" } = await req.json().catch(() => ({}));\\n  if (!isAllowedModel(model)) return NextResponse.json({ error: \"Unknown model\" }, { status: 400 });\\n  try {\\n    const res = await poe.chat.completions.create({\\n      model,\\n      messages: [{ role: \"user\", content }],\\n      max_tokens: 32,\\n      stream: false,\\n      temperature: 0,\\n    });\\n    return NextResponse.json({\\n      id: res.id,\\n      model: res.model,\\n      text: res.choices?.[0]?.message?.content ?? \"\",\\n      usage: res.usage,\\n    });\\n  } catch (e: any) {\\n    return NextResponse.json({ error: e?.message || \"request_failed\" }, { status: 500 });\\n  }\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.7 Simple client to verify setup\\nThis page fetches the health info and offers a smoke test button. You will replace it with the duel UI in Section 2.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/page.tsx\\n\"use client\";\\nimport { useEffect, useState } from \"react\";\\n\\nexport default function Home() {\\n  const [health, setHealth] = useState(null);\\n  const [loading, setLoading] = useState(false);\\n  const [output, setOutput] = useState(\"\");\\n\\n  useEffect(() => {\\n    fetch(\"/api/health\").then(r => r.json()).then(setHealth);\\n  }, []);\\n\\n  async function runSmoke() {\\n    setLoading(true);\\n    setOutput(\"\");\\n    const r = await fetch(\"/api/smoke\", { method: \"POST\", body: JSON.stringify({ content: \"pong?\" }) });\\n    const json = await r.json();\\n    setOutput(JSON.stringify(json, null, 2));\\n    setLoading(false);\\n  }\\n\\n  return (\\n    <main className=\"mx-auto max-w-2xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">ALAIN Kit • Poe Scaffold\\n      <pre className=\"rounded bg-gray-100 p-3 text-sm overflow-auto\">{JSON.stringify(health, null, 2)}\\n      <button onClick={runSmoke} disabled={loading} className=\"rounded px-4 py-2 bg-black text-white\">\\n        {loading ? \"Running…\" : \"Run smoke test\"}\\n      \\n      {output && (\\n        <pre className=\"rounded bg-gray-100 p-3 text-sm overflow-auto\">{output}\\n      )}\\n      <p className=\"text-sm text-gray-600\">Next: add model selector, duel stream, and votes.\\n    \\n  );\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 1.8 Gotchas to keep in mind\\n- Poe returns OpenAI-shaped payloads but only one choice\\n- Some clients drop the final SSE event if you assume a fixed chunk size\\n- Rate limits can spike; use a leaky bucket in later sections\\n- Keep prompts identical and cap output tokens for fair comparisons”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Store the model list in NEXT_PUBLIC_POE_COMPARISON_MODELS so non-engineers can tweak options without code changes.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Never log raw prompts that contain secrets. You will add a prompt hasher before writing telemetry.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “Keep temperature and top_p equal for both sides of a duel to reduce variance.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Poe may not emit standard rate-limit headers. Implement client-side backoff when you wire streaming.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Use short outputs during development to avoid burning credits while you iterate.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2ApbssIjAwQ"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Validator (Quality & Colab Fixer). Audit the assembled notebook for the LM Arena-style Poe chat comparison tutorial.\n",
        "\n",
        "Tasks:\n",
        "- Check schema: all sections present, markdown/code balance, ipywidgets>=8.0.0 noted in setup.\n",
        "- Validate code: static analysis for imports, missing env vars, incorrect paths. Ensure `.env.local` usage is documented (no placeholder secrets). Confirm streaming handlers close SSE, guard errors, and sanitize model IDs.\n",
        "- Run sanity tests mentally: `npm run lint`, `npm run test` (Vitest); highlight required commands.\n",
        "- Spot content issues: placeholders, TODOs, inconsistent model lists, fairness disclaimers.\n",
        "- Confirm assessments align with objectives and answer keys correct.\n",
        "- Verify telemetry covers latency, token usage, failure reasons; ensure ranking math explained + initial scores safe.\n",
        "- Ensure Colab guidance (if any) sets env vars securely or clarifies Poe is hosted (no secret logging).\n",
        "\n",
        "Deliverable (JSON via emit_validation_report):\n",
        "- status: pass|fail|needs-revision\n",
        "- summary\n",
        "- issues: list with severity, location (section/cell), message, fix_suggestion\n",
        "- follow_up_actions (if any)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsxjlSBajCAM"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“status”: “needs-revision”,\n",
        "“summary”: “Section 1 scaffolds a solid foundation, but the assembled tutorial is incomplete. Missing duel streaming API, ranking math wiring, telemetry population, assessments, and several setup items. Address the items below to reach a pass.”,\n",
        "“issues”: [\n",
        "{\n",
        "“severity”: “critical”,\n",
        "“location”: “Global notebook”,\n",
        "“message”: “Only Section 1 exists. Sections for selector, duel UI, ranking math, telemetry, safety, and wrap-up are missing.”,\n",
        "“fix_suggestion”: “Generate Sections 2–6 per design: selector + duel streaming route, voting and Elo, telemetry dashboard, ethics and safety callouts, and summary.”\n",
        "},\n",
        "{\n",
        "“severity”: “critical”,\n",
        "“location”: “Streaming layer”,\n",
        "“message”: “No Next.js API route exposes server-sent events to the client for dual model streaming.”,\n",
        "“fix_suggestion”: “Add app/api/duel/route.ts that accepts leftModel, rightModel, prompt, and params, spawns two Poe streams, multiplexes to a single ReadableStream with side tags, and closes the stream on completion or error.”\n",
        "},\n",
        "{\n",
        "“severity”: “critical”,\n",
        "“location”: “Ranking math”,\n",
        "“message”: “Elo or Bradley-Terry not wired to user votes; no initial ratings defined.”,\n",
        "“fix_suggestion”: “Add a client Elo store with initial rating 1500 per model, K-factor 16 or 24, and a BT updater for pairwise outcomes including tie handling.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “lib/poe.ts::chatStream”,\n",
        "“message”: “Streaming generator does not record total time or capture usage; no try/catch or finalization to guard stream errors.”,\n",
        "“fix_suggestion”: “Wrap streaming in try/catch; capture start, set firstTokenAt and end on completion, yield a final summary event, and surface errors to callers.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Telemetry schema vs implementation”,\n",
        "“message”: “Types include ttfbMs, totalMs, tokens, and errors, but code never populates them.”,\n",
        "“fix_suggestion”: “Populate telemetry in the duel API route. Compute TTFB and total per side, parse usage from non-stream fallback or final event if available, and attach error class names.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Next.js path aliases”,\n",
        "“message”: “Code imports use @/lib/... but no tsconfig.json paths mapping is shown.”,\n",
        "“fix_suggestion”: “Edit tsconfig.json to add baseUrl: \\\".\\\" and paths: { \\\"@/*\\\": [\\\"./*\\\"] }.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Styling”,\n",
        "“message”: “Client page uses Tailwind classes but Tailwind is not installed or configured.”,\n",
        "“fix_suggestion”: “Install Tailwind and add postcss.config.js, tailwind.config.ts, and global CSS imports or replace classes with minimal CSS.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Safety & fairness”,\n",
        "“message”: “Safety callouts exist as notes but no enforcement in code. No pre-moderation or identity masking on the UI.”,\n",
        "“fix_suggestion”: “Mask model names during voting, add a pre-filter for unsafe prompts, and show a safety banner. Add a toggle to reveal identities after vote submission.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Tooling”,\n",
        "“message”: “No lint or test commands provided.”,\n",
        "“fix_suggestion”: “Add npm run lint with ESLint and TypeScript checks, and npm run test with Vitest. Include a minimal unit test for Elo update and model guard.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “.env handling”,\n",
        "“message”: “Environment usage documented, but no explicit .gitignore guidance and no runtime validation for missing keys.”,\n",
        "“fix_suggestion”: “Ensure .env* in .gitignore and add a runtime check that throws if POE_API_KEY is missing on server routes.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Model sanitation”,\n",
        "“message”: “isAllowedModel guards exist, but API routes other than /api/smoke do not use them yet.”,\n",
        "“fix_suggestion”: “Call isAllowedModel in all routes that accept a model name and return 400 for unknown values.”\n",
        "},\n",
        "{\n",
        "“severity”: “major”,\n",
        "“location”: “Cost & rate limits”,\n",
        "“message”: “No backoff/circuit breaker around Poe rate limits or vendor errors.”,\n",
        "“fix_suggestion”: “Implement a leaky bucket per model and exponential backoff with Retry-After parsing. Expose a degraded state badge in the UI.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Docs: ipywidgets note”,\n",
        "“message”: “Validator checklist mentions ipywidgets>=8.0.0, which is irrelevant for a Next.js tutorial.”,\n",
        "“fix_suggestion”: “Add a note that Jupyter widgets are not used and that the tutorial targets Next.js.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Privacy”,\n",
        "“message”: “Prompt hashing is referenced but not implemented.”,\n",
        "“fix_suggestion”: “Add a SHA-256 prompt hasher in lib/hash.ts and store only the hash in telemetry.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Assessments”,\n",
        "“message”: “MCQs and formative checks from the design are not yet embedded into the tutorial pages.”,\n",
        "“fix_suggestion”: “Add an /learn route with the four MCQs and short exercises mapped to sections; show answer keys after submission.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Token normalization”,\n",
        "“message”: “No normalization per 1k output tokens in telemetry.”,\n",
        "“fix_suggestion”: “Compute and display score and cost metrics normalized per 1k completion tokens to reduce verbosity bias.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Legal & attribution”,\n",
        "“message”: “No Poe API terms or provider attribution in UI.”,\n",
        "“fix_suggestion”: “Add a footer that cites Poe API usage and vendor model trademarks; link to Poe terms.”\n",
        "},\n",
        "{\n",
        "“severity”: “minor”,\n",
        "“location”: “Teacher rubric”,\n",
        "“message”: “GPT-OSS-20B teacher rubric generation not implemented.”,\n",
        "“fix_suggestion”: “Add an optional app/api/judge/route.ts that prompts GPT-OSS-20B for a rubric and auto-grade, stored alongside human votes.”\n",
        "}\n",
        "],\n",
        "“follow_up_actions”: [\n",
        "“Add app/api/duel/route.ts SSE endpoint and corresponding client hook to stream both sides with telemetry.”,\n",
        "“Implement Elo and Bradley-Terry utilities with tie handling and initial ratings.”,\n",
        "“Configure Tailwind and TypeScript path aliases.”,\n",
        "“Add lib/hash.ts for prompt hashing and wire it into telemetry records.”,\n",
        "“Create /arena page with model selector, duel pane, voting panel, and safety banner that hides identities until vote.”,\n",
        "“Add ESLint, Vitest, and scripts: lint, test, typecheck, build.”,\n",
        "“Document Poe rate limits and add client backoff and a UI degraded-state indicator.”,\n",
        "“Embed assessments and answer keys under an /learn route and confirm alignment with objectives.”,\n",
        "“Add a footer with Poe terms, model attribution, and bias disclosure.”,\n",
        "“Add a privacy note and confirm .env.local is excluded from version control.”\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y2QkVzUk4Yk"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Semantic Reviewer. Perform a narrative/content audit after technical validation for the LM Arena-style Poe comparison tutorial.\n",
        "\n",
        "Focus:\n",
        "- Clarity: explanations of ranking math, telemetry, and Poe-specific behaviour understandable to target audience.\n",
        "- Tone & style: consistent instructor voice, actionable tips, avoids marketing fluff.\n",
        "- Inclusivity & ethics: fairness disclaimers, bias discussion, safety mitigations present.\n",
        "- Pedagogical flow: each section starts with objective, ends with takeaway; transitions coherent.\n",
        "- Terminology accuracy: differentiate GPT-OSS teacher vs. compared models, note Poe vs. OpenAI endpoints.\n",
        "- Assessment quality: ensure MCQs test meaningful concepts, explanations teach remediation.\n",
        "- Callouts: verify Tip/Warning usage adds value.\n",
        "\n",
        "Output (emit_semantic_review):\n",
        "- status (ready | needs-edits)\n",
        "- strengths (3+ bullet strings)\n",
        "- weaknesses (issues needing fix, with location references)\n",
        "- recommended_edits (actionable suggestions)\n",
        "- style_score (1-5)\n",
        "- pedagogy_score (1-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4BBplvhlkcL"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“status”: “needs-edits”,\n",
        "“strengths”: [\n",
        "“Clear motivation and scope in Section 1; readers understand why an LM Arena on Poe matters.”,\n",
        "“Accurate Poe endpoint usage with OpenAI SDK and clean env surface (POE_API_KEY, NEXT_PUBLIC_*).”,\n",
        "“Concise, actionable Tips and Warnings that flag fairness, streaming quirks, and rate limits.”,\n",
        "“Good separation of concerns: lib/poe.ts for client, lib/models.ts for model guard, lib/types.ts for telemetry schema.”\n",
        "],\n",
        "“weaknesses”: [\n",
        "{\n",
        "“location”: “Section 1.0–1.8 narrative”,\n",
        "“message”: “No explicit learning objectives or section takeaway statements at start and end. Readers lack clear success criteria.”,\n",
        "“impact”: “Flow and expectations feel implicit; harder to self-assess progress.”\n",
        "},\n",
        "{\n",
        "“location”: “Ranking math (missing Section)”,\n",
        "“message”: “No narrative explanation of Elo vs Bradley–Terry with a numeric worked example and tie handling.”,\n",
        "“impact”: “Engineers may implement incorrect updates or misinterpret ratings.”\n",
        "},\n",
        "{\n",
        "“location”: “Telemetry explanation (Section 1.4 + missing Telemetry section)”,\n",
        "“message”: “Types define ttfbMs, totalMs, token counts, and error fields, but the text does not explain formulas, normalization per 1k tokens, or common failure classes.”,\n",
        "“impact”: “Readers cannot reason about fairness or compare models across verbosity.”\n",
        "},\n",
        "{\n",
        "“location”: “Poe-specific behavior (Section 1.2, 1.8)”,\n",
        "“message”: “Streaming behavior lacks an explicit explanation of [DONE] semantics, SSE parsing pitfalls, and differences in rate-limit headers.”,\n",
        "“impact”: “Implementations may drop final tokens or mishandle backoff.”\n",
        "},\n",
        "{\n",
        "“location”: “Inclusivity & ethics (global)”,\n",
        "“message”: “Safety and bias are mentioned, but no rater guidance, identity masking workflow, or post-vote reveal policy is spelled out.”,\n",
        "“impact”: “Risk of biased votes and unsafe prompts without mitigations.”\n",
        "},\n",
        "{\n",
        "“location”: “Terminology (global narrative)”,\n",
        "“message”: “The text mentions a \"highlight teacher\" but does not define the teacher vs. compared models consistently, nor when the teacher auto-judges vs. humans vote.”,\n",
        "“impact”: “Confusion about roles and when to trust auto-judgment.”\n",
        "},\n",
        "{\n",
        "“location”: “Assessments (absent in tutorial pages)”,\n",
        "“message”: “MCQs and formative checks from the design are not embedded near the relevant sections and lack remediation pointers in the tutorial itself.”,\n",
        "“impact”: “Learners cannot validate understanding within the flow.”\n",
        "},\n",
        "{\n",
        "“location”: “Callouts (Section 1.8)”,\n",
        "“message”: “Callouts are helpful but miss a quick fairness checklist and concrete parameter presets for apples-to-apples evaluation.”,\n",
        "“impact”: “Readers may drift from consistent settings across duels.”\n",
        "}\n",
        "],\n",
        "“recommended_edits”: [\n",
        "“Add an \"Objectives\" block at the top of each section and a \"Takeaways\" block at the end, mirroring the Lesson Architect SMART goals.”,\n",
        "“Introduce a dedicated \"Terminology\" callout in Section 1 that defines Teacher model (GPT-OSS-20B), Compared models, Duel, Vote, Auto-judge, Human-override.”,\n",
        "“Create a Ranking Math section that explains Elo and Bradley–Terry with a worked example: compute expected scores, update after Left win, Right win, Tie; include tie coefficient choice and rationale.”,\n",
        "“Expand Telemetry section to define TTFB, Total latency, Tokens per second, Error classes. Show formulas for normalization per 1k completion tokens and why it reduces verbosity bias.”,\n",
        "“Add an explicit Poe Streaming note: describe SSE message shape, [DONE] terminator, common client pitfalls, and how to guard parser finalization.”,\n",
        "“Insert a Fairness & Safety panel before the duel UI: enforce identical temperature, top_p, and max_tokens; mask identities until vote; provide rater guidance bullets; add a prompt safety checklist.”,\n",
        "“Embed the four MCQs as an /learn route and link to them from the end of the relevant sections. After each answer, show a short remediation paragraph and a link back to the code anchor.”,\n",
        "“Provide a short example prompt set with domains (code, reasoning, writing). Add a note on prompt stratification and sampling to avoid domain skew.”,\n",
        "“Add a footer disclaimer that results reflect models accessed through Poe, with attribution and a link to Poe terms.”,\n",
        "“Include a mini fairness checklist callout: parameters parity, identity masking, prompt hashing, refusal tracking, and normalization per 1k.”\n",
        "],\n",
        "“style_score”: 4,\n",
        "“pedagogy_score”: 3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThatUVxUlppE"
      },
      "source": [
        "System:\n",
        "You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-09-19\n",
        "Reasoning: high\n",
        "\n",
        "Developer:\n",
        "You are the ALAIN-Kit Orchestrator. Package the validated sections into the final notebook artifacts for the LM Arena-style Poe comparison tutorial with GPT-OSS-20B as teacher.\n",
        "\n",
        "Responsibilities:\n",
        "- Assemble cells in section order, inject global metadata header (generation timestamp, teacher model, provider Poe, model roster).\n",
        "- Ensure setup cell installs `openai>=4.57.0`, `ipywidgets>=8.0.0`, `zustand`, `d3` (for charts) if referenced.\n",
        "- Prepend environment guidance cell:\n",
        "  ```bash\n",
        "  !pip install openai>=4.57.0 ipywidgets>=8.0.0\n",
        "  %env OPENAI_BASE_URL=https://api.poe.com/v1\n",
        "  %env OPENAI_API_KEY=$POE_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTxkdQHNmBSA"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“notebook_json”: {\n",
        "“nbformat”: 4,\n",
        "“nbformat_minor”: 5,\n",
        "“metadata”: {\n",
        "“kernelspec”: {\n",
        "“name”: “python3”,\n",
        "“display_name”: “Python 3”\n",
        "},\n",
        "“language_info”: {\n",
        "“name”: “python”,\n",
        "“version”: “3.x”\n",
        "},\n",
        "“alain”: {\n",
        "“title”: “ALAIN-Kit LM Arena on Poe - Tutorial Notebook”,\n",
        "“generated_at”: “2025-09-19T14:00:00-05:00”,\n",
        "“provider”: “Poe”,\n",
        "“default_teacher”: “GPT-OSS-20B”,\n",
        "“model_roster”: [\n",
        "“GPT-5”,\n",
        "“Claude-Sonnet-4”,\n",
        "“Grok-4”,\n",
        "“Gemini-2.5-Pro”,\n",
        "“Llama-3.1-405B”,\n",
        "“Mixtral-8x22B”,\n",
        "“GPT-OSS-20B”\n",
        "]\n",
        "}\n",
        "},\n",
        "“cells”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “# ALAIN-Kit LM Arena on Poe\\n\\nProvider: Poe\\n\\nDefault teacher: GPT-OSS-20B\\n\\nModel roster: GPT-5, Claude-Sonnet-4, Grok-4, Gemini-2.5-Pro, Llama-3.1-405B, Mixtral-8x22B, GPT-OSS-20B\\n\\nGenerated at: 2025-09-19 14:00 America/Chicago\\n\\nPurpose: Build a Next.js App Router project that compares two Poe models side by side, streams outputs, records a vote, updates ratings, and logs telemetry.\\n\\nConfidence: 82 percent”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## Environment guidance\\nUse secrets. Do not hardcode API keys.\\n\\n- Local app: put keys in .env.local and add .env* to .gitignore.\\n- Colab: store secrets with Colab Secrets or the environment pane. Avoid printing keys.\\n\\nThe notebook ships setup for quick API smoke tests and for storing code files you will paste into your Next.js app.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“tags”: [\n",
        "“setup”\n",
        "]\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “!pip -q install \"openai>=4.57.0\" \"ipywidgets>=8.0.0\"\\n%env OPENAI_BASE_URL=https://api.poe.com/v1\\n# Set OPENAI_API_KEY through Colab secrets or the environment panel\\n# Example (do not run in shared environments):\\n# %env OPENAI_API_KEY=YOUR_POE_API_KEY”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## Node dependencies for your Next.js app\\nRun in your project root:\\n\\nbash\\nnpm install openai zustand d3\\n# Optional UI libs often used: tailwindcss @tanstack/react-query recharts framer-motion lucide-react clsx\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## Reproducibility seed\\nThe Next.js app logic will not use Python, but we set a seed here to keep any notebook-side random checks stable.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “import os, random, time\\nrandom.seed(42)\\nos.environ.get(\"OPENAI_BASE_URL\"), time.time()”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## Section 1 - Scaffold overview\\nYou will create a Poe client, a model guard, telemetry types, a health route, a smoke route, and a simple home page. Paste the following files into your Next.js project.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “lib/poe.ts”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// lib/poe.ts\\nimport OpenAI from \"openai\";\\n\\nexport const poe = new OpenAI({\\n  apiKey: process.env.POE_API_KEY!,\\n  baseURL: \"https://api.poe.com/v1\",\\n});\\n\\nexport type ChatMessage = { role: \"system\" | \"user\" | \"assistant\"; content: string };\\n\\nexport async function chatOnce(model: string, messages: ChatMessage[]) {\\n  const res = await poe.chat.completions.create({ model, messages, stream: false, max_tokens: 512 });\\n  const choice = res.choices?.[0];\\n  return {\\n    text: choice?.message?.content ?? \"\",\\n    usage: res.usage,\\n    id: res.id,\\n    model: res.model,\\n  };\\n}\\n\\nexport async function* chatStream(model: string, messages: ChatMessage[]) {\\n  // Client-side helpers can call this via an API route that proxies SSE\\n  const stream = await poe.chat.completions.create({ model, messages, stream: true, max_tokens: 512 });\\n  const start = Date.now();\\n  let firstTokenAt: number | null = null;\\n  try {\\n    for await (const part of stream) {\\n      const delta = part.choices?.[0]?.delta?.content ?? \"\";\\n      if (delta && firstTokenAt === null) firstTokenAt = Date.now();\\n      yield { delta, ttfbMs: firstTokenAt ? firstTokenAt - start : null };\\n    }\\n  } finally {\\n    // stream closes when iterator completes\\n  }\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “lib/models.ts”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// lib/models.ts\\nconst DEFAULTS = [\\n  \"GPT-5\",\\n  \"Claude-Sonnet-4\",\\n  \"Grok-4\",\\n  \"Gemini-2.5-Pro\",\\n  \"Llama-3.1-405B\",\\n  \"Mixtral-8x22B\",\\n  \"GPT-OSS-20B\",\\n] as const;\\n\\nexport const COMPARISON_MODELS: readonly string[] = (process.env.NEXT_PUBLIC_POE_COMPARISON_MODELS?.split(\",\")\\n  .map(s => s.trim())\\n  .filter(Boolean) || DEFAULTS) as readonly string[];\\n\\nexport type PoeModel = typeof DEFAULTS[number] | (string & {});\\n\\nexport function isAllowedModel(m: string): m is PoeModel {\\n  return COMPARISON_MODELS.includes(m);\\n}\\n\\nexport const DEFAULT_MODEL: PoeModel = (process.env.NEXT_PUBLIC_POE_DEFAULT_MODEL || \"GPT-OSS-20B\") as PoeModel;”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “lib/types.ts”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// lib/types.ts\\nexport type Telemetry = {\\n  promptHash: string;\\n  leftModel: string;\\n  rightModel: string;\\n  params: { temperature: number; top_p: number; max_tokens: number };\\n  left: { ttfbMs: number | null; totalMs: number | null; promptTokens?: number; completionTokens?: number; error?: string };\\n  right: { ttfbMs: number | null; totalMs: number | null; promptTokens?: number; completionTokens?: number; error?: string };\\n};\\n\\nexport type VoteOutcome = \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\";\\n\\nexport type DuelRecord = Telemetry & {\\n  vote: VoteOutcome | null;\\n  voterNote?: string;\\n  createdAt: string;\\n};”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “app/api/health/route.ts”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// app/api/health/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { COMPARISON_MODELS, DEFAULT_MODEL } from \"@/lib/models\";\\n\\nexport async function GET() {\\n  const ok = Boolean(process.env.POE_API_KEY);\\n  return NextResponse.json({ ok, hasKey: ok, defaultModel: DEFAULT_MODEL, models: COMPARISON_MODELS });\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “app/api/smoke/route.ts”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// app/api/smoke/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { isAllowedModel, DEFAULT_MODEL } from \"@/lib/models\";\\n\\nexport async function POST(req: Request) {\\n  const { model = DEFAULT_MODEL, content = \"Say ‘pong’\" } = await req.json().catch(() => ({}));\\n  if (!isAllowedModel(model)) return NextResponse.json({ error: \"Unknown model\" }, { status: 400 });\\n  try {\\n    const res = await poe.chat.completions.create({\\n      model,\\n      messages: [{ role: \"user\", content }],\\n      max_tokens: 32,\\n      stream: false,\\n      temperature: 0,\\n    });\\n    return NextResponse.json({ id: res.id, model: res.model, text: res.choices?.[0]?.message?.content ?? \"\", usage: res.usage });\\n  } catch (e: any) {\\n    return NextResponse.json({ error: e?.message || \"request_failed\" }, { status: 500 });\\n  }\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“metadata”: {\n",
        "“filename”: “app/page.tsx”\n",
        "},\n",
        "“execution_count”: null,\n",
        "“outputs”: [],\n",
        "“source”: “// app/page.tsx\\n\"use client\";\\nimport { useEffect, useState } from \"react\";\\n\\nexport default function Home() {\\n  const [health, setHealth] = useState(null);\\n  const [loading, setLoading] = useState(false);\\n  const [output, setOutput] = useState(\"\");\\n\\n  useEffect(() => { fetch(\"/api/health\").then(r => r.json()).then(setHealth); }, []);\\n\\n  async function runSmoke() {\\n    setLoading(true);\\n    setOutput(\"\");\\n    const r = await fetch(\"/api/smoke\", { method: \"POST\", body: JSON.stringify({ content: \"pong?\" }) });\\n    const json = await r.json();\\n    setOutput(JSON.stringify(json, null, 2));\\n    setLoading(false);\\n  }\\n\\n  return (\\n    <main className=\"mx-auto max-w-2xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">ALAIN Kit • Poe Scaffold\\n      <pre className=\"rounded bg-gray-100 p-3 text-sm overflow-auto\">{JSON.stringify(health, null, 2)}\\n      <button onClick={runSmoke} disabled={loading} className=\"rounded px-4 py-2 bg-black text-white\">\\n        {loading ? \"Running…\" : \"Run smoke test\"}\\n      \\n      {output && <pre className=\"rounded bg-gray-100 p-3 text-sm overflow-auto\">{output}}\\n      <p className=\"text-sm text-gray-600\">Next: add model selector, duel stream, and votes.\\n    \\n  );\\n}”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## QA summary and execution order\\n1. Install Node deps in your app folder.\\n2. Create .env.local with POE_API_KEY, NEXT_PUBLIC_POE_DEFAULT_MODEL, and NEXT_PUBLIC_POE_COMPARISON_MODELS.\\n3. Add tsconfig.json path alias: paths: { \\\"@/*\\\": [\\\"./*\\\"] } and baseUrl: \\\".\\\".\\n4. Add Tailwind or adjust styling.\\n5. Run npm run dev, open /, hit the smoke test.\\n\\n### Lint and tests\\nAdd scripts:\\n\\njson\\n{\\n  \\\"scripts\\\": {\\n    \\\"lint\\\": \\\"next lint\\\",\\n    \\\"typecheck\\\": \\\"tsc --noEmit\\\",\\n    \\\"test\\\": \\\"vitest run\\\",\\n    \\\"dev\\\": \\\"next dev\\\"\\n  }\\n}\\n\\n\\nCreate a small Vitest suite for model guard and a unit test for Elo once you add ranking.\\n\\nManual streaming check will land in Section 2 when you add the duel SSE route.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## Fairness, safety, and terminology\\n- Teacher vs compared models: Teacher is GPT-OSS-20B on Poe. It generates rubrics or auto-judges. Humans still vote. Compared models are the pair in each duel.\\n- Mask identities until a vote records. Reveal identities only after.\\n- Keep identical temperature, top_p, and max_tokens on both sides.\\n- Log refusals and unsafe outputs as a separate category.\\n- Normalize any quality score per 1k completion tokens to reduce verbosity bias.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“metadata”: {},\n",
        "“source”: “## How to use in Playground\\n- Paste each prompt System text into the Playground System field.\\n- Paste the Developer block into the first message as needed.\\n- Send the task input as the User message, for example: request Section 2 to generate the duel SSE route and the voting UI.”\n",
        "}\n",
        "]\n",
        "},\n",
        "“summary_markdown”: “# Release notes\\n\\nConfidence: 82 percent\\n\\nWhat is included\\n- Global metadata header with provider, teacher, and roster\\n- Environment guidance cells for Colab and for a local .env.local\\n- Section 1 scaffold: Poe client, model guard, telemetry types, health and smoke API routes, starter page\\n- QA summary with scripts and manual checks\\n\\nKnown gaps to complete in next sections\\n- Duel SSE API that streams two models concurrently and records telemetry\\n- Voting panel, Elo or Bradley Terry updates, and a leaderboard\\n- Telemetry population and normalization per 1k tokens\\n- Safety banner, identity masking, rater guidance\\n- Assessments and remediation in an /learn route”,\n",
        "“metadata”: {\n",
        "“model_roster”: [\n",
        "“GPT-5”,\n",
        "“Claude-Sonnet-4”,\n",
        "“Grok-4”,\n",
        "“Gemini-2.5-Pro”,\n",
        "“Llama-3.1-405B”,\n",
        "“Mixtral-8x22B”,\n",
        "“GPT-OSS-20B”\n",
        "],\n",
        "“default_teacher”: “GPT-OSS-20B”,\n",
        "“runtime”: “Notebook: Python 3. Colab optional. App: Next.js 14, Node 20.”,\n",
        "“generated_at”: “2025-09-19T14:00:00-05:00”\n",
        "},\n",
        "“qa_log”: {\n",
        "“schema”: {\n",
        "“sections_present”: [\n",
        "“Section 1 scaffold”\n",
        "],\n",
        "“markdown_code_balance”: “approx 60-40 in Section 1”,\n",
        "“ipywidgets_note”: “Installed. Not used directly in app.”\n",
        "},\n",
        "“code_validation”: {\n",
        "“imports_ok”: true,\n",
        "“env_vars_documented”: [\n",
        "“POE_API_KEY”,\n",
        "“NEXT_PUBLIC_POE_DEFAULT_MODEL”,\n",
        "“NEXT_PUBLIC_POE_COMPARISON_MODELS”\n",
        "],\n",
        "“paths”: “@/* requires tsconfig paths mapping”,\n",
        "“streaming_handlers”: “Present in lib/poe.ts helper, final SSE route still pending”\n",
        "},\n",
        "“sanity_tests”: {\n",
        "“lint”: “scripts suggested”,\n",
        "“test”: “Vitest suggested”,\n",
        "“manual_streaming_check”: “Pending until duel SSE route is added”\n",
        "},\n",
        "“content_checks”: {\n",
        "“model_list_consistency”: true,\n",
        "“fairness_disclaimer”: “Included at end”,\n",
        "“placeholders”: “No secrets. Some features marked as next sections.”\n",
        "},\n",
        "“assessments_alignment”: “Assessments will be added with later sections”,\n",
        "“telemetry_coverage”: “Types defined. Population pending.”,\n",
        "“ranking_math”: “To be added in Section 3”,\n",
        "“colab_guidance”: “Secrets guidance provided. Base URL set. No key printed.”\n",
        "}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9P8orA4oQcu"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 2,\n",
        "“title”: “Model Selector, Duel SSE API, Pairwise Streamed UI, and Voting”,\n",
        "“estimated_tokens”: 2400,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 2. Duel streaming, selector, and voting\\nYou will add a server-sent events API that streams two Poe models side-by-side, a model selector with an enum guard, a duel UI that accumulates streamed tokens, and a voting panel that updates Elo ratings in client state. Telemetry for TTFB and total latency is emitted with the final done event.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 2.1 SSE duel API overview\\n- Endpoint: GET /api/duel with query params: left, right, prompt, temperature, top_p, max_tokens.\\n- Response: text/event-stream with events:\\n  - chunk → { side: \\\"left\\\"|\\\"right\\\", delta: string }\\n  - error → { side?: string, message: string }\\n  - done → { promptHash, params, left: { ttfbMs, totalMs, usage? }, right: { ... } }\\n- Fairness: both sides receive identical generation params.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/duel/route.ts\\nimport { NextRequest } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { isAllowedModel } from \"@/lib/models\";\\nimport crypto from \"crypto\";\\n\\nexport const runtime = \"nodejs\";\\n\\nfunction sseHeaders() {\\n  return {\\n    \"Content-Type\": \"text/event-stream; charset=utf-8\",\\n    \"Cache-Control\": \"no-cache, no-transform\",\\n    Connection: \"keep-alive\",\\n    \"X-Accel-Buffering\": \"no\",\\n  } as Record<string, string>;\\n}\\n\\nfunction writeEvent(writer: WritableStreamDefaultWriter, name: string, data: any) {\\n  const enc = new TextEncoder();\\n  const payload = event: ${name}\\\\ndata: ${JSON.stringify(data)}\\\\n\\\\n;\\n  return writer.write(enc.encode(payload));\\n}\\n\\nexport async function GET(req: NextRequest) {\\n  const url = new URL(req.url);\\n  const left = url.searchParams.get(\"left\") || \"\";\\n  const right = url.searchParams.get(\"right\") || \"\";\\n  const promptRaw = url.searchParams.get(\"prompt\") || \"\";\\n  const temperature = Number(url.searchParams.get(\"temperature\") ?? 0.2);\\n  const top_p = Number(url.searchParams.get(\"top_p\") ?? 1);\\n  const max_tokens = Number(url.searchParams.get(\"max_tokens\") ?? 512);\\n\\n  if (!isAllowedModel(left) || !isAllowedModel(right)) {\\n    return new Response(\"Unknown model\", { status: 400 });\\n  }\\n  const prompt = decodeURIComponent(promptRaw);\\n  const promptHash = crypto.createHash(\"sha256\").update(prompt).digest(\"hex\").slice(0, 16);\\n  const messages = [{ role: \"user\" as const, content: prompt }];\\n\\n  const { readable, writable } = new TransformStream<Uint8Array, Uint8Array>();\\n  const writer = writable.getWriter();\\n\\n  const summary: any = {\\n    promptHash,\\n    params: { temperature, top_p, max_tokens },\\n    left: { ttfbMs: null, totalMs: null },\\n    right: { ttfbMs: null, totalMs: null },\\n  };\\n\\n  async function runSide(side: \"left\" | \"right\", model: string) {\\n    const start = Date.now();\\n    let firstTokenAt: number | null = null;\\n    try {\\n      const stream: any = await poe.chat.completions.create({\\n        model,\\n        messages,\\n        temperature,\\n        top_p,\\n        max_tokens,\\n        stream: true,\\n      });\\n      for await (const part of stream) {\\n        const delta = part?.choices?.[0]?.delta?.content ?? \"\";\\n        if (delta) {\\n          if (firstTokenAt === null) firstTokenAt = Date.now();\\n          await writeEvent(writer, \"chunk\", { side, delta });\\n        }\\n      }\\n      let usage: any = undefined;\\n      try {\\n        const final = await stream.finalChatCompletion?.();\\n        usage = final?.usage;\\n      } catch {}\\n      const totalMs = Date.now() - start;\\n      const ttfbMs = firstTokenAt ? firstTokenAt - start : null;\\n      summary[side] = { …summary[side], ttfbMs, totalMs, usage };\\n    } catch (e: any) {\\n      await writeEvent(writer, \"error\", { side, message: e?.message || \"stream_error\" });\\n      const totalMs = Date.now() - start;\\n      summary[side] = { …summary[side], ttfbMs: null, totalMs, error: String(e?.message || e) };\\n    }\\n  }\\n\\n  (async () => {\\n    try {\\n      await Promise.all([runSide(\"left\", left), runSide(\"right\", right)]);\\n      await writeEvent(writer, \"done\", summary);\\n    } catch (e: any) {\\n      await writeEvent(writer, \"error\", { message: e?.message || \"duel_failed\" });\\n    } finally {\\n      try { await writer.close(); } catch {}\\n    }\\n  })();\\n\\n  return new Response(readable, { headers: sseHeaders() });\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 2.2 Elo utilities and prompt hashing\\nWe maintain a lightweight Elo implementation with tie handling. \"Both bad\" records no rating change. Ratings default to 1500 and K=24 by default.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/elo.ts\\nexport type Outcome = \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\";\\n\\nexport function expectedScore(ra: number, rb: number) {\\n  return 1 / (1 + Math.pow(10, (rb - ra) / 400));\\n}\\n\\nexport function updateElo(ra: number, rb: number, outcome: Outcome, K = 24) {\\n  if (outcome === \"BOTH_BAD\") return [ra, rb] as const;\\n  const ea = expectedScore(ra, rb);\\n  const eb = 1 - ea;\\n  const sa = outcome === \"LEFT\" ? 1 : outcome === \"RIGHT\" ? 0 : 0.5;\\n  const sb = 1 - sa;\\n  const na = ra + K * (sa - ea);\\n  const nb = rb + K * (sb - eb);\\n  return [Math.round(na), Math.round(nb)] as const;\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/hash.ts\\nimport crypto from \"crypto\";\\n\\nexport function sha256Hex(input: string) {\\n  return crypto.createHash(\"sha256\").update(input).digest(\"hex\");\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 2.3 Ratings store and duel history\\nWe use Zustand to store ratings and recent duels. You will extend this in the telemetry section to persist to a backend if desired.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/store.ts\\n\"use client\";\\nimport { create } from \"zustand\";\\nimport { updateElo, Outcome } from \"./elo\";\\n\\nexport type DuelSummary = {\\n  promptHash: string;\\n  leftModel: string;\\n  rightModel: string;\\n  left: { ttfbMs: number | null; totalMs: number | null; completionTokens?: number; promptTokens?: number };\\n  right: { ttfbMs: number | null; totalMs: number | null; completionTokens?: number; promptTokens?: number };\\n};\\n\\ntype RatingsState = {\\n  ratings: Record<string, number>;\\n  history: { summary: DuelSummary; outcome: Outcome; at: string }[];\\n  ensure(model: string): void;\\n  record(outcome: Outcome, leftModel: string, rightModel: string, summary: DuelSummary): void;\\n  reset(): void;\\n};\\n\\nexport const useRatings = create((set, get) => ({\\n  ratings: {},\\n  history: [],\\n  ensure(model: string) {\\n    const ratings = { …get().ratings };\\n    if (!(model in ratings)) ratings[model] = 1500;\\n    set({ ratings });\\n  },\\n  record(outcome, leftModel, rightModel, summary) {\\n    const ratings = { …get().ratings };\\n    if (!(leftModel in ratings)) ratings[leftModel] = 1500;\\n    if (!(rightModel in ratings)) ratings[rightModel] = 1500;\\n    const [na, nb] = updateElo(ratings[leftModel], ratings[rightModel], outcome);\\n    ratings[leftModel] = na;\\n    ratings[rightModel] = nb;\\n    const history = [{ summary, outcome, at: new Date().toISOString() }, …get().history].slice(0, 50);\\n    set({ ratings, history });\\n  },\\n  reset() { set({ ratings: {}, history: [] }); },\\n}));\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 2.4 Duel UI page\\n- Model selectors read from the public env list.\\n- Prompt box, params, and a \"Start duel\" button.\\n- Two streamed panes mask identities during voting. Identities reveal after vote.\\n- Voting panel records outcome and updates ratings.\\n- Telemetry summary shows TTFB and total duration per side.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/arena/page.tsx\\n\"use client\";\\nimport { useEffect, useMemo, useRef, useState } from \"react\";\\nimport { COMPARISON_MODELS } from \"@/lib/models\";\\nimport { useRatings } from \"@/lib/store\";\\n\\nfunction Pane({ title, text }: { title: string; text: string }) {\\n  return (\\n    <div className=\"flex-1 rounded border p-3 min-h-[200px]\">\\n      <div className=\"text-xs uppercase tracking-wide text-gray-500 mb-2\">{title}\\n      <div className=\"whitespace-pre-wrap text-sm leading-6\">{text || \"\"}\\n    \\n  );\\n}\\n\\nexport default function ArenaPage() {\\n  const models = COMPARISON_MODELS as string[];\\n  const [leftModel, setLeftModel] = useState(models[0] || \"GPT-OSS-20B\");\\n  const [rightModel, setRightModel] = useState(models[1] || models[0] || \"GPT-5\");\\n  const [prompt, setPrompt] = useState(\"Write a concise explanation of the difference between HTTP/1.1 and HTTP/2.\");\\n  const [temperature, setTemperature] = useState(0.2);\\n  const [topP, setTopP] = useState(1);\\n  const [maxTokens, setMaxTokens] = useState(512);\\n  const [leftText, setLeftText] = useState(\"\");\\n  const [rightText, setRightText] = useState(\"\");\\n  const [streaming, setStreaming] = useState(false);\\n  const [masked, setMasked] = useState(true);\\n  const [summary, setSummary] = useState(null);\\n  const esRef = useRef<EventSource | null>(null);\\n  const ratings = useRatings(s => s.ratings);\\n  const ensure = useRatings(s => s.ensure);\\n  const record = useRatings(s => s.record);\\n\\n  useEffect(() => { ensure(leftModel); ensure(rightModel); }, [leftModel, rightModel, ensure]);\\n\\n  const maskedTitles = useMemo(() => ({\\n    left: masked ? \"Model A\" : leftModel,\\n    right: masked ? \"Model B\" : rightModel,\\n  }), [masked, leftModel, rightModel]);\\n\\n  function stopStream() {\\n    esRef.current?.close?.();\\n    esRef.current = null;\\n    setStreaming(false);\\n  }\\n\\n  async function startDuel() {\\n    stopStream();\\n    setLeftText(\"\");\\n    setRightText(\"\");\\n    setSummary(null);\\n    setStreaming(true);\\n    const url = /api/duel?left=${encodeURIComponent(leftModel)}&right=${encodeURIComponent(rightModel)}&prompt=${encodeURIComponent(prompt)}&temperature=${temperature}&top_p=${topP}&max_tokens=${maxTokens};\\n    const es = new EventSource(url);\\n    esRef.current = es;\\n\\n    es.addEventListener(\"chunk\", (e: MessageEvent) => {\\n      try {\\n        const { side, delta } = JSON.parse(e.data);\\n        if (side === \"left\") setLeftText(prev => prev + delta);\\n        else if (side === \"right\") setRightText(prev => prev + delta);\\n      } catch {}\\n    });\\n\\n    es.addEventListener(\"error\", (e) => {\\n      console.error(\"SSE error\", e);\\n      stopStream();\\n    });\\n\\n    es.addEventListener(\"done\", (e: MessageEvent) => {\\n      try {\\n        const data = JSON.parse(e.data);\\n        setSummary(data);\\n      } catch {}\\n      stopStream();\\n    });\\n  }\\n\\n  function vote(outcome: \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\") {\\n    if (!summary) return;\\n    const duelSummary = {\\n      promptHash: summary.promptHash,\\n      leftModel,\\n      rightModel,\\n      left: {\\n        ttfbMs: summary.left?.ttfbMs ?? null,\\n        totalMs: summary.left?.totalMs ?? null,\\n        completionTokens: summary.left?.usage?.completion_tokens,\\n        promptTokens: summary.left?.usage?.prompt_tokens,\\n      },\\n      right: {\\n        ttfbMs: summary.right?.ttfbMs ?? null,\\n        totalMs: summary.right?.totalMs ?? null,\\n        completionTokens: summary.right?.usage?.completion_tokens,\\n        promptTokens: summary.right?.usage?.prompt_tokens,\\n      },\\n    };\\n    record(outcome, leftModel, rightModel, duelSummary);\\n    setMasked(false); // reveal identities post-vote\\n  }\\n\\n  const leftRating = ratings[leftModel] ?? 1500;\\n  const rightRating = ratings[rightModel] ?? 1500;\\n\\n  return (\\n    <main className=\"mx-auto max-w-5xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Duel Arena\\n\\n      <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\\n        \\n          <label className=\"block text-sm\">Left model\\n          <select value={leftModel} onChange={e => setLeftModel(e.target.value)} className=\"w-full border rounded p-2\">\\n            {models.map(m => {m})}\\n          \\n        \\n        \\n          <label className=\"block text-sm\">Right model\\n          <select value={rightModel} onChange={e => setRightModel(e.target.value)} className=\"w-full border rounded p-2\">\\n            {models.map(m => {m})}\\n          \\n        \\n      \\n\\n      \\n        <label className=\"block text-sm\">Prompt\\n        <textarea value={prompt} onChange={e => setPrompt(e.target.value)} className=\"w-full border rounded p-2 h-28\" />\\n      \\n\\n      <div className=\"grid grid-cols-3 gap-3\">\\n        \\n          <label className=\"block text-xs\">temperature\\n          <input type=\"number\" step=\"0.1\" min={0} max={1} value={temperature} onChange={e => setTemperature(Number(e.target.value))} className=\"w-full border rounded p-2\" />\\n        \\n        \\n          <label className=\"block text-xs\">top_p\\n          <input type=\"number\" step=\"0.1\" min={0} max={1} value={topP} onChange={e => setTopP(Number(e.target.value))} className=\"w-full border rounded p-2\" />\\n        \\n        \\n          <label className=\"block text-xs\">max_tokens\\n          <input type=\"number\" step=\"1\" min={32} max={2048} value={maxTokens} onChange={e => setMaxTokens(Number(e.target.value))} className=\"w-full border rounded p-2\" />\\n        \\n      \\n\\n      <div className=\"flex items-center gap-3\">\\n        <button onClick={startDuel} disabled={streaming} className=\"rounded px-4 py-2 bg-black text-white\">{streaming ? \"Streaming…\" : \"Start duel\"}\\n        <button onClick={() => setMasked(v => !v)} className=\"rounded px-3 py-2 border\">{masked ? \"Reveal after vote\" : \"Mask names\"}\\n        <div className=\"text-sm text-gray-600\">Ratings • {leftModel}: {leftRating} | {rightModel}: {rightRating}\\n      \\n\\n      <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\\n        \\n        \\n      \\n\\n      <div className=\"flex gap-2\">\\n        <button onClick={() => vote(\"LEFT\")} disabled={!summary} className=\"rounded px-3 py-2 border\">Left wins\\n        <button onClick={() => vote(\"RIGHT\")} disabled={!summary} className=\"rounded px-3 py-2 border\">Right wins\\n        <button onClick={() => vote(\"TIE\")} disabled={!summary} className=\"rounded px-3 py-2 border\">Tie\\n        <button onClick={() => vote(\"BOTH_BAD\")} disabled={!summary} className=\"rounded px-3 py-2 border\">Both bad\\n      \\n\\n      {summary && (\\n        <div className=\"rounded border p-3 text-sm\">\\n          <div className=\"font-medium mb-1\">Telemetry\\n          <pre className=\"whitespace-pre-wrap\">{JSON.stringify(summary, null, 2)}\\n          <div className=\"text-xs text-gray-600\">Prompt hash: {summary.promptHash}\\n        \\n      )}\\n\\n      <div className=\"text-xs text-gray-600\">\\n        Safety: identities remain hidden until you vote. Keep identical params across both sides for fairness. Track refusals as \"Both bad\" when appropriate.\\n      \\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 2.5 Manual check\\n1. Start dev server and open /arena.\\n2. Select two models and run a duel with the default prompt.\\n3. Confirm both panes stream tokens. If one model errors, an error event should appear and the duel should still complete.\\n4. Vote and verify ratings update and identities reveal.\\n5. Inspect the telemetry summary for ttfbMs and totalMs per side.”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “For very long prompts that risk URL length, implement a POST NDJSON variant at /api/duel and stream with ReadableStream on the client. Use the same event payloads.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Poe rate limits can spike. Avoid starting many duels in parallel. Add a small client-side queue if multiple users will test at once.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “EventSource only supports GET. This section uses GET with query params for simplicity. Switch to POST streaming if you need larger payloads.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Keep temperature, top_p, and max_tokens identical across both models. Differences can invalidate votes.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Treat \"Both bad\" as no rating change to avoid inflating or deflating ratings due to refusals or unsafe outputs.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “The SDK’s finalChatCompletion() is used to pull usage after a stream ends. If unavailable, skip usage or add a non-stream probe request sparingly.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv4rm3bboRnx"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 3,\n",
        "“title”: “Ranking Math, Leaderboard, and History”,\n",
        "“estimated_tokens”: 2600,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 3. Ranking math, leaderboard, and history\\nYou will wire votes into stable ratings, compute simple statistics, and render a leaderboard. You will use Elo as the primary ranking and keep a Bradley–Terry style logistic updater for analysis. You will also persist ratings and duel history in the browser so results survive reloads.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.1 Elo recap and tie handling\\nElo models each model’s latent strength as a single number. For a left model with rating Ra and a right model with rating Rb, the expected left score is:\\n\\nEa = 1 / (1 + 10^((Rb - Ra)/400))\\n\\nUpdate each rating with K * (S - E) where S is the observed score (1 for left win, 0.5 for tie, 0 for right win). Treat \"Both bad\" as no change. You can tune K between 16 and 32 depending on how quickly you want ratings to move.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/elo.ts (extended)\\nexport type Outcome = \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\";\\n\\nexport function expectedScore(ra: number, rb: number) {\\n  return 1 / (1 + Math.pow(10, (rb - ra) / 400));\\n}\\n\\nexport function updateElo(ra: number, rb: number, outcome: Outcome, K = 24) {\\n  if (outcome === \"BOTH_BAD\") return [ra, rb] as const;\\n  const ea = expectedScore(ra, rb);\\n  const sa = outcome === \"LEFT\" ? 1 : outcome === \"RIGHT\" ? 0 : 0.5;\\n  const sb = 1 - sa;\\n  const na = ra + K * (sa - ea);\\n  const nb = rb + K * (sb - (1 - ea));\\n  return [Math.round(na), Math.round(nb)] as const;\\n}\\n\\nexport function recomputeElo(\\n  pairs: { left: string; right: string; outcome: Outcome }[],\\n  initial: Record<string, number> = {},\\n  K = 24\\n) {\\n  const ratings: Record<string, number> = { …initial };\\n  for (const p of pairs) {\\n    if (!(p.left in ratings)) ratings[p.left] = 1500;\\n    if (!(p.right in ratings)) ratings[p.right] = 1500;\\n    const [na, nb] = updateElo(ratings[p.left], ratings[p.right], p.outcome, K);\\n    ratings[p.left] = na;\\n    ratings[p.right] = nb;\\n  }\\n  return ratings;\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.2 Bradley–Terry style logistic update (optional analysis)\\nYou can also maintain a logistic strength score Ri per model and update via gradient ascent on the pairwise likelihood. This treats a tie as 0.5. Use it to cross-check Elo. You do not need to display this score to users.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/bt.ts\\nexport type Outcome = \"LEFT\" | \"RIGHT\" | \"TIE\" | \"BOTH_BAD\";\\n\\nfunction pLeft(rl: number, rr: number) {\\n  return 1 / (1 + Math.exp(rr - rl));\\n}\\n\\nexport function btUpdate(rl: number, rr: number, outcome: Outcome, lr = 0.05) {\\n  if (outcome === \"BOTH_BAD\") return [rl, rr] as const;\\n  const y = outcome === \"LEFT\" ? 1 : outcome === \"RIGHT\" ? 0 : 0.5;\\n  const pij = pLeft(rl, rr);\\n  const grad = y - pij; // logistic gradient\\n  return [rl + lr * grad, rr - lr * grad] as const;\\n}\\n\\nexport function recomputeBT(\\n  pairs: { left: string; right: string; outcome: Outcome }[],\\n  initial: Record<string, number> = {},\\n  lr = 0.05\\n) {\\n  const r: Record<string, number> = { …initial };\\n  for (const p of pairs) {\\n    if (!(p.left in r)) r[p.left] = 0; // BT uses log-odds scale\\n    if (!(p.right in r)) r[p.right] = 0;\\n    const [nl, nr] = btUpdate(r[p.left], r[p.right], p.outcome, lr);\\n    r[p.left] = nl;\\n    r[p.right] = nr;\\n  }\\n  return r;\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.3 Persist ratings and history\\nYou will update the Zustand store to persist state in localStorage. You also add import and export helpers.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/store.ts (persisted)\\n\"use client\";\\nimport { create } from \"zustand\";\\nimport { persist } from \"zustand/middleware\";\\nimport { updateElo, Outcome } from \"./elo\";\\n\\nexport type DuelSummary = {\\n  promptHash: string;\\n  leftModel: string;\\n  rightModel: string;\\n  left: { ttfbMs: number | null; totalMs: number | null; completionTokens?: number; promptTokens?: number };\\n  right: { ttfbMs: number | null; totalMs: number | null; completionTokens?: number; promptTokens?: number };\\n};\\n\\ntype RatingsState = {\\n  ratings: Record<string, number>;\\n  history: { summary: DuelSummary; outcome: Outcome; at: string }[];\\n  ensure(model: string): void;\\n  record(outcome: Outcome, leftModel: string, rightModel: string, summary: DuelSummary): void;\\n  reset(): void;\\n  exportJSON(): string;\\n  importJSON(payload: string): void;\\n};\\n\\nexport const useRatings = create()(\\n  persist(\\n    (set, get) => ({\\n      ratings: {},\\n      history: [],\\n      ensure(model: string) {\\n        const ratings = { …get().ratings };\\n        if (!(model in ratings)) ratings[model] = 1500;\\n        set({ ratings });\\n      },\\n      record(outcome, leftModel, rightModel, summary) {\\n        const ratings = { …get().ratings };\\n        if (!(leftModel in ratings)) ratings[leftModel] = 1500;\\n        if (!(rightModel in ratings)) ratings[rightModel] = 1500;\\n        const [na, nb] = updateElo(ratings[leftModel], ratings[rightModel], outcome);\\n        ratings[leftModel] = na;\\n        ratings[rightModel] = nb;\\n        const history = [{ summary, outcome, at: new Date().toISOString() }, …get().history].slice(0, 200);\\n        set({ ratings, history });\\n      },\\n      reset() { set({ ratings: {}, history: [] }); },\\n      exportJSON() {\\n        return JSON.stringify({ ratings: get().ratings, history: get().history }, null, 2);\\n      },\\n      importJSON(payload: string) {\\n        const parsed = JSON.parse(payload);\\n        if (!parsed || typeof parsed !== \"object\") return;\\n        set({ ratings: parsed.ratings || {}, history: parsed.history || [] });\\n      },\\n    }),\\n    { name: \"alain-arena-store\" }\\n  )\\n);\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.4 Metrics and bootstrap confidence intervals\\nYou will compute basic win rates and a lightweight bootstrap over the duel list to estimate a confidence interval for Elo. You can trigger this on demand to avoid heavy computation on every render.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/metrics.ts\\nimport { Outcome, recomputeElo } from \"./elo\";\\n\\nexport type Pair = { left: string; right: string; outcome: Outcome };\\n\\nexport function toPairs(history: { summary: any; outcome: Outcome }[]): Pair[] {\\n  return history.map(h => ({ left: h.summary.leftModel, right: h.summary.rightModel, outcome: h.outcome }));\\n}\\n\\nexport function counts(history: { summary: any; outcome: Outcome }[]) {\\n  const played: Record<string, number> = {};\\n  const wins: Record<string, number> = {};\\n  const ties: Record<string, number> = {};\\n  const losses: Record<string, number> = {};\\n  for (const h of history) {\\n    const L = h.summary.leftModel; const R = h.summary.rightModel;\\n    played[L] = (played[L] || 0) + 1; played[R] = (played[R] || 0) + 1;\\n    if (h.outcome === \"LEFT\") { wins[L] = (wins[L] || 0) + 1; losses[R] = (losses[R] || 0) + 1; }\\n    else if (h.outcome === \"RIGHT\") { wins[R] = (wins[R] || 0) + 1; losses[L] = (losses[L] || 0) + 1; }\\n    else if (h.outcome === \"TIE\") { ties[L] = (ties[L] || 0) + 1; ties[R] = (ties[R] || 0) + 1; }\\n  }\\n  const winRate: Record<string, number> = {};\\n  Object.keys(played).forEach(m => {\\n    const w = wins[m] || 0, t = ties[m] || 0, p = played[m] || 1;\\n    winRate[m] = (w + 0.5 * t) / p;\\n  });\\n  return { played, wins, losses, ties, winRate };\\n}\\n\\nexport function bootstrapEloCI(\\n  history: { summary: any; outcome: Outcome }[],\\n  models: string[],\\n  B = 200,\\n  K = 24\\n) {\\n  if (history.length === 0) return Object.fromEntries(models.map(m => [m, { lo: NaN, hi: NaN }])) as Record<string, { lo: number; hi: number }>;\\n  const pairs = toPairs(history);\\n  const results: Record<string, number[]> = Object.fromEntries(models.map(m => [m, [] as number[]]));\\n  for (let b = 0; b < B; b++) {\\n    const sample: typeof pairs = [];\\n    for (let i = 0; i < pairs.length; i++) sample.push(pairs[Math.floor(Math.random() * pairs.length)]);\\n    const r = recomputeElo(sample, {}, K);\\n    models.forEach(m => results[m].push(r[m] ?? 1500));\\n  }\\n  const ci: Record<string, { lo: number; hi: number }> = {};\\n  for (const m of models) {\\n    const arr = results[m].sort((a, b) => a - b);\\n    const lo = arr[Math.floor(0.16 * arr.length)] ?? NaN; // ~68 percent interval\\n    const hi = arr[Math.floor(0.84 * arr.length)] ?? NaN;\\n    ci[m] = { lo: Math.round(lo), hi: Math.round(hi) };\\n  }\\n  return ci;\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.5 Leaderboard page\\nThis page renders ratings, basic counts, and optional confidence intervals. It also exposes export and import for the current state.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/leaderboard/page.tsx\\n\"use client\";\\nimport { useMemo, useState } from \"react\";\\nimport { useRatings } from \"@/lib/store\";\\nimport { counts, bootstrapEloCI } from \"@/lib/metrics\";\\n\\nexport default function LeaderboardPage() {\\n  const ratings = useRatings(s => s.ratings);\\n  const history = useRatings(s => s.history);\\n  const exportJSON = useRatings(s => s.exportJSON);\\n  const importJSON = useRatings(s => s.importJSON);\\n  const reset = useRatings(s => s.reset);\\n\\n  const models = useMemo(() => Object.keys(ratings).sort(), [ratings]);\\n  const stats = useMemo(() => counts(history), [history]);\\n\\n  const rows = useMemo(() => {\\n    return Object.entries(ratings)\\n      .map(([model, rating]) => ({\\n        model,\\n        rating,\\n        played: stats.played[model] || 0,\\n        winRate: stats.winRate[model] || 0,\\n      }))\\n      .sort((a, b) => b.rating - a.rating);\\n  }, [ratings, stats]);\\n\\n  const [ci, setCI] = useState<Record<string, { lo: number; hi: number }> | null>(null);\\n\\n  function computeCI() {\\n    const res = bootstrapEloCI(history, models, 200, 24);\\n    setCI(res);\\n  }\\n\\n  function download() {\\n    const blob = new Blob([exportJSON()], { type: \"application/json\" });\\n    const url = URL.createObjectURL(blob);\\n    const a = document.createElement(\"a\");\\n    a.href = url; a.download = \"arena-state.json\"; a.click();\\n    URL.revokeObjectURL(url);\\n  }\\n\\n  async function upload(ev: any) {\\n    const file = ev.target.files?.[0];\\n    if (!file) return;\\n    const text = await file.text();\\n    importJSON(text);\\n  }\\n\\n  return (\\n    <main className=\"mx-auto max-w-4xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Leaderboard\\n\\n      <div className=\"flex gap-2 items-center\">\\n        <button onClick={computeCI} className=\"rounded px-3 py-2 border\">Compute CI\\n        <button onClick={download} className=\"rounded px-3 py-2 border\">Export\\n        <label className=\"rounded px-3 py-2 border cursor-pointer\">Import<input type=\"file\" accept=\"application/json\" className=\"hidden\" onChange={upload} />\\n        <button onClick={reset} className=\"rounded px-3 py-2 border\">Reset\\n        <div className=\"text-xs text-gray-600\">CI uses bootstrap resampling. Expect noise with few duels.\\n      \\n\\n      <table className=\"w-full text-sm border-collapse\">\\n        \\n          <tr className=\"text-left border-b\">\\n            <th className=\"py-2\">Rank\\n            <th className=\"py-2\">Model\\n            <th className=\"py-2\">Rating\\n            <th className=\"py-2\">68% CI\\n            <th className=\"py-2\">Played\\n            <th className=\"py-2\">Win rate\\n          \\n        \\n        \\n          {rows.map((r, idx) => (\\n            <tr key={r.model} className=\"border-b\">\\n              <td className=\"py-2\">{idx + 1}\\n              <td className=\"py-2\">{r.model}\\n              <td className=\"py-2\">{r.rating}\\n              <td className=\"py-2\">{ci ? ${ci[r.model]?.lo ?? \\\"?\\\"}–${ci[r.model]?.hi ?? \\\"?\\\"} : \"—\"}\\n              <td className=\"py-2\">{r.played}\\n              <td className=\"py-2\">{(r.winRate * 100).toFixed(1)}%\\n            \\n          ))}\\n        \\n      \\n\\n      <div className=\"text-xs text-gray-600\">\\n        Notes: Ratings start at 1500. Elo updates use K=24. CI reports a bootstrap interval. Treat results with caution when played < 20.\\n      \\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.6 Worked numeric example\\nAssume Left 1500 and Right 1500. Expected Ea equals 0.5. If Left wins, new ratings become 1512 and 1488 with K=24. If you record a tie, both stay at 1500.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 3.7 Manual check\\n1. Run a few duels on /arena and vote.\\n2. Open /leaderboard and confirm ratings and counts update.\\n3. Click Compute CI and verify the table shows intervals. Small histories will produce wide intervals.\\n4. Export JSON, refresh the page, Import the JSON, and confirm the leaderboard restores.”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Small sample sizes create volatile ratings. Avoid public claims until each model has at least 20 to 50 duels.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Keep GPT-OSS-20B in the duel rotation as a teacher reference. You can pin a fixed pairing schedule that includes the teacher in 1 of every 3 duels.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “Bootstrap CI runs on the client. Use a low number of resamples during development to keep the UI responsive.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “If you see rating collapse due to many ties, reduce K to 16 or seed initial ratings closer to your expectations.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Do not mix different generation parameters in the same leaderboard. Differences in temperature or max tokens will bias ratings.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IorhKi-f2l-E"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 4,\n",
        "“title”: “Telemetry Capture, Normalization, and Dashboard”,\n",
        "“estimated_tokens”: 2400,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 4. Telemetry capture, normalization, and dashboard\\nYou will record latency, token usage, and failure reasons for each duel. You will normalize latency per 1k completion tokens to reduce verbosity bias and visualize model performance on a telemetry dashboard.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 4.1 What to log and why\\n- Latency: time to first byte (TTFB) and total time per side\\n- Tokens: prompt and completion tokens from final usage when available\\n- Throughput: tokens per second\\n- Normalization: milliseconds per 1k completion tokens\\n- Errors and refusals: error class names and refusal flags\\n- Privacy: store prompt hashes only, never raw prompts”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/telemetry.ts\\n\"use client\";\\nimport { create } from \"zustand\";\\nimport { persist } from \"zustand/middleware\";\\n\\nexport type SideTelem = {\\n  model: string;\\n  ttfbMs: number | null;\\n  totalMs: number | null;\\n  promptTokens?: number;\\n  completionTokens?: number;\\n  error?: string;\\n  refusal?: boolean; // heuristic from rendered text\\n  tps?: number | null; // tokens per second\\n  msPer1k?: number | null; // normalized latency\\n};\\n\\nexport type TelemetryRecord = {\\n  promptHash: string;\\n  params: { temperature: number; top_p: number; max_tokens: number };\\n  left: SideTelem;\\n  right: SideTelem;\\n  at: string; // ISO\\n};\\n\\ntype TelemetryState = {\\n  records: TelemetryRecord[];\\n  add: (rec: TelemetryRecord) => void;\\n  reset: () => void;\\n  exportJSON: () => string;\\n  importJSON: (payload: string) => void;\\n  byModel: () => Record<string, TelemetryRecord[]>;\\n  summary: () => Record<string, {\\n    samples: number;\\n    avgTTFB: number | null;\\n    avgTotalMs: number | null;\\n    avgCompletionTok: number | null;\\n    avgTPS: number | null;\\n    avgMsPer1k: number | null;\\n    refusalRate: number;\\n    errorRate: number;\\n  }>;\\n};\\n\\nfunction safeAvg(arr: (number | null | undefined)[]) {\\n  const xs = arr.filter((v): v is number => typeof v === \"number\" && isFinite(v));\\n  if (xs.length === 0) return null;\\n  return xs.reduce((a, b) => a + b, 0) / xs.length;\\n}\\n\\nexport function detectRefusal(text: string) {\\n  const s = (text || \"\").toLowerCase();\\n  const phrases = [\\n    \"i can’t assist\", \"i cannot help\", \"i’m unable to\", \"as an ai\", \"i will not\", \"not appropriate\", \"refuse\", \"violates policy\"\\n  ];\\n  return phrases.some(p => s.includes(p));\\n}\\n\\nfunction enrich(side: Omit<SideTelem, \"tps\" | \"msPer1k\">): SideTelem {\\n  const total = side.totalMs ?? null;\\n  const ctok = side.completionTokens ?? 0;\\n  const tps = total && ctok > 0 ? ctok / (total / 1000) : null;\\n  const msPer1k = total && ctok > 0 ? total / (ctok / 1000) : null;\\n  return { …side, tps, msPer1k };\\n}\\n\\nexport const useTelemetry = create()(\\n  persist(\\n    (set, get) => ({\\n      records: [],\\n      add(rec) {\\n        // compute derived metrics per side before storing\\n        const left = enrich(rec.left);\\n        const right = enrich(rec.right);\\n        const full: TelemetryRecord = { …rec, left, right };\\n        set({ records: [full, …get().records].slice(0, 500) });\\n      },\\n      reset() { set({ records: [] }); },\\n      exportJSON() { return JSON.stringify({ records: get().records }, null, 2); },\\n      importJSON(payload: string) {\\n        const parsed = JSON.parse(payload);\\n        if (!parsed || typeof parsed !== \"object\" || !Array.isArray(parsed.records)) return;\\n        set({ records: parsed.records });\\n      },\\n      byModel() {\\n        const map: Record<string, TelemetryRecord[]> = {};\\n        for (const r of get().records) {\\n          map[r.left.model] = map[r.left.model] || [];\\n          map[r.right.model] = map[r.right.model] || [];\\n          map[r.left.model].push(r);\\n          map[r.right.model].push(r);\\n        }\\n        return map;\\n      },\\n      summary() {\\n        const map = get().byModel();\\n        const out: Record<string, any> = {};\\n        for (const m of Object.keys(map)) {\\n          const rs = map[m];\\n          const sides: SideTelem[] = rs.flatMap(r => [r.left, r.right]).filter(s => s.model === m);\\n          const samples = sides.length;\\n          const avgTTFB = safeAvg(sides.map(s => s.ttfbMs));\\n          const avgTotalMs = safeAvg(sides.map(s => s.totalMs));\\n          const avgCompletionTok = safeAvg(sides.map(s => s.completionTokens ?? null));\\n          const avgTPS = safeAvg(sides.map(s => s.tps ?? null));\\n          const avgMsPer1k = safeAvg(sides.map(s => s.msPer1k ?? null));\\n          const refusalRate = samples ? sides.filter(s => s.refusal).length / samples : 0;\\n          const errorRate = samples ? sides.filter(s => s.error).length / samples : 0;\\n          out[m] = { samples, avgTTFB, avgTotalMs, avgCompletionTok, avgTPS, avgMsPer1k, refusalRate, errorRate };\\n        }\\n        return out;\\n      },\\n    }),\\n    { name: \"alain-telemetry-store\" }\\n  )\\n);\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 4.2 Capture telemetry at duel completion\\nWhen the done event arrives, add a telemetry record that includes refusal heuristics from the displayed texts. Do not store raw prompts.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/arena/page.tsx (add telemetry wiring)\\n\"use client\";\\nimport { useEffect, useMemo, useRef, useState } from \"react\";\\nimport { COMPARISON_MODELS } from \"@/lib/models\";\\nimport { useRatings } from \"@/lib/store\";\\nimport { useTelemetry, detectRefusal } from \"@/lib/telemetry\";\\n\\n// … keep Pane and component shell from Section 2\\n\\nexport default function ArenaPage() {\\n  // existing state\\n  const addTelem = useTelemetry(s => s.add);\\n  // … existing hooks and state\\n\\n  async function startDuel() {\\n    // … existing code\\n    es.addEventListener(\"done\", (e: MessageEvent) => {\\n      try {\\n        const data = JSON.parse(e.data);\\n        setSummary(data);\\n        // Add telemetry immediately using rendered texts and usage\\n        addTelem({\\n          promptHash: data.promptHash,\\n          params: data.params,\\n          left: {\\n            model: leftModel,\\n            ttfbMs: data.left?.ttfbMs ?? null,\\n            totalMs: data.left?.totalMs ?? null,\\n            promptTokens: data.left?.usage?.prompt_tokens,\\n            completionTokens: data.left?.usage?.completion_tokens,\\n            error: data.left?.error,\\n            refusal: detectRefusal(leftText),\\n          },\\n          right: {\\n            model: rightModel,\\n            ttfbMs: data.right?.ttfbMs ?? null,\\n            totalMs: data.right?.totalMs ?? null,\\n            promptTokens: data.right?.usage?.prompt_tokens,\\n            completionTokens: data.right?.usage?.completion_tokens,\\n            error: data.right?.error,\\n            refusal: detectRefusal(rightText),\\n          },\\n          at: new Date().toISOString(),\\n        });\\n      } catch {}\\n      stopStream();\\n    });\\n  }\\n\\n  // … rest unchanged\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 4.3 Telemetry dashboard page\\nRenders a sortable table and a D3 bar chart of normalized latency by model. The chart shows average milliseconds per 1k completion tokens.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/telemetry/page.tsx\\n\"use client\";\\nimport { useMemo, useRef, useEffect, useState } from \"react\";\\nimport { useTelemetry } from \"@/lib/telemetry\";\\nimport * as d3 from \"d3\";\\n\\nfunction BarChart({ data }: { data: { model: string; value: number }[] }) {\\n  const ref = useRef<SVGSVGElement | null>(null);\\n  useEffect(() => {\\n    const svg = d3.select(ref.current!);\\n    svg.selectAll(\"*\").remove();\\n    const width = 760, height = 320, margin = { top: 20, right: 20, bottom: 60, left: 60 };\\n    svg.attr(\"viewBox\", 0 0 ${width} ${height});\\n    const x = d3.scaleBand().domain(data.map(d => d.model)).range([margin.left, width - margin.right]).padding(0.2);\\n    const y = d3.scaleLinear().domain([0, d3.max(data, d => d.value)! || 1]).nice().range([height - margin.bottom, margin.top]);\\n    const g = svg.append(\"g\");\\n    g.selectAll(\"rect\").data(data).enter().append(\"rect\")\\n      .attr(\"x\", d => x(d.model)!)\\n      .attr(\"y\", d => y(d.value))\\n      .attr(\"width\", x.bandwidth())\\n      .attr(\"height\", d => y(0) - y(d.value));\\n    g.append(\"g\").attr(\"transform\", translate(0,${height - margin.bottom})).call(d3.axisBottom(x)).selectAll(\"text\").attr(\"transform\", \"rotate(20)\").style(\"text-anchor\", \"start\");\\n    g.append(\"g\").attr(\"transform\", translate(${margin.left},0)).call(d3.axisLeft(y));\\n    g.append(\"text\").attr(\"x\", width / 2).attr(\"y\", margin.top).attr(\"text-anchor\", \"middle\").text(\"Avg ms per 1k completion tokens\");\\n  }, [data]);\\n  return <svg ref={ref} className=\"w-full h-[340px]\" />;\\n}\\n\\nexport default function TelemetryPage() {\\n  const records = useTelemetry(s => s.records);\\n  const summary = useTelemetry(s => s.summary)();\\n  const reset = useTelemetry(s => s.reset);\\n  const exportJSON = useTelemetry(s => s.exportJSON);\\n  const importJSON = useTelemetry(s => s.importJSON);\\n\\n  const rows = useMemo(() => Object.entries(summary)\\n    .map(([model, s]) => ({ model, …s }))\\n    .sort((a, b) => (a.avgMsPer1k ?? Infinity) - (b.avgMsPer1k ?? Infinity)), [summary]);\\n\\n  const chartData = useMemo(() => rows\\n    .filter(r => Number.isFinite(r.avgMsPer1k))\\n    .map(r => ({ model: r.model, value: r.avgMsPer1k! })), [rows]);\\n\\n  function download() {\\n    const blob = new Blob([exportJSON()], { type: \"application/json\" });\\n    const url = URL.createObjectURL(blob);\\n    const a = document.createElement(\"a\"); a.href = url; a.download = \"telemetry.json\"; a.click(); URL.revokeObjectURL(url);\\n  }\\n  async function upload(ev: any) {\\n    const f = ev.target.files?.[0]; if (!f) return; importJSON(await f.text());\\n  }\\n\\n  const [showRaw, setShowRaw] = useState(false);\\n\\n  return (\\n    <main className=\"mx-auto max-w-5xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Telemetry\\n      <div className=\"flex gap-2 items-center\">\\n        <button onClick={download} className=\"rounded px-3 py-2 border\">Export\\n        <label className=\"rounded px-3 py-2 border cursor-pointer\">Import<input type=\"file\" accept=\"application/json\" className=\"hidden\" onChange={upload} />\\n        <button onClick={reset} className=\"rounded px-3 py-2 border\">Reset\\n        <label className=\"flex items-center gap-2 text-sm text-gray-700\"><input type=\"checkbox\" checked={showRaw} onChange={e => setShowRaw(e.target.checked)} /> Show raw\\n      \\n\\n      <div className=\"overflow-auto\">\\n        <table className=\"w-full text-sm border-collapse\">\\n          \\n            <tr className=\"text-left border-b\">\\n              <th className=\"py-2\">Model\\n              <th className=\"py-2\">Samples\\n              <th className=\"py-2\">Avg TTFB ms\\n              <th className=\"py-2\">Avg Total ms\\n              <th className=\"py-2\">Avg comp tok\\n              <th className=\"py-2\">Avg tok/sec\\n              <th className=\"py-2\">Avg ms per 1k\\n              <th className=\"py-2\">Refusal rate\\n              <th className=\"py-2\">Error rate\\n            \\n          \\n          \\n            {rows.map(r => (\\n              <tr key={r.model} className=\"border-b\">\\n                <td className=\"py-2\">{r.model}\\n                <td className=\"py-2\">{r.samples}\\n                <td className=\"py-2\">{r.avgTTFB?.toFixed(0) ?? \"\"}\\n                <td className=\"py-2\">{r.avgTotalMs?.toFixed(0) ?? \"\"}\\n                <td className=\"py-2\">{r.avgCompletionTok?.toFixed(1) ?? \"\"}\\n                <td className=\"py-2\">{r.avgTPS?.toFixed(2) ?? \"\"}\\n                <td className=\"py-2\">{r.avgMsPer1k?.toFixed(0) ?? \"\"}\\n                <td className=\"py-2\">{(r.refusalRate * 100).toFixed(1)}%\\n                <td className=\"py-2\">{(r.errorRate * 100).toFixed(1)}%\\n              \\n            ))}\\n          \\n        \\n      \\n\\n      \\n\\n      {showRaw && (\\n        <details className=\"rounded border p-3 text-sm\">\\n          Raw telemetry records\\n          <pre className=\"overflow-auto whitespace-pre-wrap\">{JSON.stringify(records, null, 2)}\\n        \\n      )}\\n\\n      <div className=\"text-xs text-gray-600\">Normalization reduces bias from verbose models. Treat ms per 1k completion tokens as the primary throughput metric across models.\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 4.4 Manual check\\n1. Run a few duels on /arena and wait for done events.\\n2. Open /telemetry and confirm a table row appears per model with non-null averages.\\n3. Verify refusal flags by testing a safety-related prompt and observing refusal rate changes.\\n4. Export and import telemetry JSON to validate portability.”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Use normalized ms per 1k completion tokens when comparing speed across models. It is robust to verbosity differences.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Usage tokens may be undefined on some streams. Keep code resilient and avoid dividing by zero.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “Heuristic refusal detection is a proxy. For higher fidelity, call the teacher to classify refusal without storing raw text.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “If prompts get large, prefer POST streaming for the duel API to avoid oversized URLs. Store only hashes in telemetry.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Do not publish raw prompts or completions. Treat telemetry as potentially sensitive even without content.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgOXrpX13CZs"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 5,\n",
        "“title”: “Safety, Fairness, Rater Guidance, and Teacher Auto-Judge”,\n",
        "“estimated_tokens”: 2600,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 5. Safety, fairness, rater guidance, and teacher auto-judge\\nYou will add prompt pre-moderation, a rater guidance panel, identity masking policy, and an optional teacher auto-judge using GPT-OSS-20B on Poe that produces a rubric and a JSON verdict. Treat the teacher as an assistant to human votes, not a replacement.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.1 Safety utilities and moderation endpoint\\nWe implement a lightweight local pre-moderation check plus an optional teacher classification. The endpoint never stores raw prompts and returns only allow or block with reasons.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/safety.ts\\nexport type ModerationResult = { allow: boolean; reasons: string[]; usedTeacher?: boolean };\\n\\nconst BAD_PATTERNS = [\\n  /how to make (?:a )?bomb/i,\\n  /credit card number generator/i,\\n  /child sexual/i,\\n  /explicit gore/i,\\n  /doxx/i,\\n];\\n\\nexport function localModerate(text: string): ModerationResult {\\n  const reasons: string[] = [];\\n  for (const re of BAD_PATTERNS) {\\n    if (re.test(text)) reasons.push(matched:${re.source});\\n  }\\n  return { allow: reasons.length === 0, reasons };\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/moderate/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { localModerate } from \"@/lib/safety\";\\n\\nexport async function POST(req: Request) {\\n  const body = await req.json().catch(() => ({}));\\n  const { prompt = \"\", useTeacher = false, teacherModel = process.env.NEXT_PUBLIC_POE_DEFAULT_MODEL || \"GPT-OSS-20B\" } = body;\\n  const local = localModerate(String(prompt));\\n  if (!local.allow && !useTeacher) return NextResponse.json({ …local, usedTeacher: false });\\n\\n  if (useTeacher) {\\n    try {\\n      const sys = \"You are a concise safety classifier. Output strict JSON: {\\\"allow\\\":boolean,\\\"reasons\\\":[string]}. Block if content requests illegal activity, sexual minors, doxxing, or explicit gore.\";\\n      const res = await poe.chat.completions.create({\\n        model: teacherModel,\\n        stream: false,\\n        messages: [\\n          { role: \"system\", content: sys },\\n          { role: \"user\", content: Classify:\\\\n${prompt} },\\n        ],\\n        max_tokens: 128,\\n        temperature: 0,\\n      });\\n      const text = res.choices?.[0]?.message?.content || \"{}\";\\n      const parsed = JSON.parse(text);\\n      const allow = Boolean(parsed.allow);\\n      const reasons = Array.isArray(parsed.reasons) ? parsed.reasons.map(String) : [];\\n      return NextResponse.json({ allow, reasons, usedTeacher: true });\\n    } catch (e: any) {\\n      // On failure, fall back to local decision\\n      return NextResponse.json({ …local, usedTeacher: false, error: String(e?.message || e) });\\n    }\\n  }\\n\\n  return NextResponse.json({ …local, usedTeacher: false });\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.2 Teacher auto-judge endpoint\\nGiven a prompt and two responses, ask GPT-OSS-20B to produce a rubric, scores, and a verdict in strict JSON. The server parses and returns the structured result.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/judge/route.ts\\nimport { NextResponse } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\n\\nfunction extractJSON(s: string) {\\n  // Best effort JSON extraction to guard stray text\\n  const start = s.indexOf(\"{\");\\n  const end = s.lastIndexOf(\"}\");\\n  if (start >= 0 && end > start) {\\n    try { return JSON.parse(s.slice(start, end + 1)); } catch {}\\n  }\\n  return null;\\n}\\n\\nexport async function POST(req: Request) {\\n  const { prompt = \"\", left = \"\", right = \"\", teacherModel = process.env.NEXT_PUBLIC_POE_DEFAULT_MODEL || \"GPT-OSS-20B\" } = await req.json().catch(() => ({}));\\n  if (!prompt || !left || !right) return NextResponse.json({ error: \"missing_fields\" }, { status: 400 });\\n  const sys = You are a careful, concise judge. Return strict JSON only with keys: rubric, scores, verdict, reasons, safety_flags.\\n- rubric: {criteria:[{name, weight, desc}]}\\n- scores: {left:number, right:number, tie_possible:boolean}\\n- verdict: \\\"LEFT\\\"|\\\"RIGHT\\\"|\\\"TIE\\\"|\\\"BOTH_BAD\\\"\\n- reasons: string[] concise bullet points\\n- safety_flags: string[]\\nScoring guidance: prioritize correctness, relevance, clarity. Penalize unsafe content. Keep weights sum near 1.;\\n\\n  const user = PROMPT\\\\n${prompt}\\\\n\\\\nLEFT\\\\n${left}\\\\n\\\\nRIGHT\\\\n${right};\\n  try {\\n    const res = await poe.chat.completions.create({\\n      model: teacherModel,\\n      stream: false,\\n      messages: [\\n        { role: \"system\", content: sys },\\n        { role: \"user\", content: user },\\n      ],\\n      max_tokens: 512,\\n      temperature: 0,\\n    });\\n    const text = res.choices?.[0]?.message?.content || \"{}\";\\n    const json = extractJSON(text) || {};\\n    return NextResponse.json({ judge: json });\\n  } catch (e: any) {\\n    return NextResponse.json({ error: String(e?.message || e) }, { status: 500 });\\n  }\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.3 Rater guidance and fairness checklist components\\nDisplay clear guidance before voting. Keep model identities masked until the vote records. Offer a post-vote reveal.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// components/RaterGuide.tsx\\n\"use client\";\\nexport function RaterGuide() {\\n  return (\\n    <aside className=\"rounded border p-3 text-sm space-y-2 bg-gray-50\">\\n      <div className=\"font-medium\">Rater guidance\\n      <ul className=\"list-disc ml-5 space-y-1\">\\n        Evaluate correctness, relevance, clarity. Ignore brand and style quirks.\\n        Do not consider output length except where brevity harms clarity.\\n        If both are unsafe, off-topic, or obviously wrong, choose Both bad.\\n        Use Tie only when both are roughly equal in quality.\\n        Keep parameters identical across both sides for fairness.\\n      \\n      <div className=\"text-xs text-gray-600\">Identities stay hidden until you submit a vote.\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// components/FairnessChecklist.tsx\\n\"use client\";\\nexport function FairnessChecklist({ params }: { params: { temperature: number; top_p: number; max_tokens: number } }) {\\n  return (\\n    <aside className=\"rounded border p-3 text-sm space-y-2\">\\n      <div className=\"font-medium\">Fairness checklist\\n      <ul className=\"list-disc ml-5 space-y-1\">\\n        Same temperature, top_p, and max_tokens on both sides.\\n        Prompt hashed and not stored in plaintext.\\n        Refusals tracked separately from quality votes.\\n        Normalize latency by tokens for throughput comparisons.\\n      \\n      <div className=\"text-xs text-gray-600\">Params: t={params.temperature} top_p={params.top_p} max={params.max_tokens}\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// components/AttributionFooter.tsx\\n\"use client\";\\nexport function AttributionFooter() {\\n  return (\\n    <footer className=\"text-xs text-gray-600 mt-8\">\\n      Models are accessed through Poe’s OpenAI-compatible API. Capabilities and limits may differ from vendor direct endpoints. See Poe terms.\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.4 Wire moderation and judge into the Arena UI\\n- Before starting a duel, call /api/moderate. Block or warn if disallowed.\\n- Add an Auto-judge button that posts to /api/judge with the prompt and both responses, then renders the rubric and verdict below the voting panel.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/arena/page.tsx (augment Section 2 UI)\\n\"use client\";\\nimport { useEffect, useMemo, useRef, useState } from \"react\";\\nimport { COMPARISON_MODELS } from \"@/lib/models\";\\nimport { useRatings } from \"@/lib/store\";\\nimport { useTelemetry, detectRefusal } from \"@/lib/telemetry\";\\nimport { RaterGuide } from \"@/components/RaterGuide\";\\nimport { FairnessChecklist } from \"@/components/FairnessChecklist\";\\nimport { AttributionFooter } from \"@/components/AttributionFooter\";\\n\\n// … keep Pane and existing code from Section 2 and 4\\n\\nexport default function ArenaPage() {\\n  // existing state\\n  const [moderation, setModeration] = useState(null);\\n  const [judge, setJudge] = useState(null);\\n\\n  async function startDuel() {\\n    // Moderation first\\n    const mod = await fetch(\"/api/moderate\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ prompt, useTeacher: false }) }).then(r => r.json());\\n    setModeration(mod);\\n    if (!mod?.allow) {\\n      alert(\"Prompt blocked by safety check: \" + (mod?.reasons?.join(\", \") || \"unsafe\"));\\n      return;\\n    }\\n    // … continue existing startDuel logic to open EventSource\\n  }\\n\\n  async function runJudge() {\\n    setJudge(null);\\n    const res = await fetch(\"/api/judge\", {\\n      method: \"POST\",\\n      headers: { \"Content-Type\": \"application/json\" },\\n      body: JSON.stringify({ prompt, left: leftText, right: rightText }),\\n    }).then(r => r.json());\\n    setJudge(res?.judge || { error: res?.error || \"no_judge\" });\\n  }\\n\\n  return (\\n    <main className=\"mx-auto max-w-5xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Duel Arena\\n\\n      {/* Guidance and fairness /}\\n      \\n      <FairnessChecklist params={{ temperature, top_p: topP, max_tokens: maxTokens }} />\\n\\n      {/ existing selectors, prompt, params, buttons, panes /}\\n\\n      <div className=\"flex gap-2\">\\n        {/ voting buttons unchanged */}\\n        <button onClick={runJudge} disabled={!leftText || !rightText} className=\"rounded px-3 py-2 border\">Auto-judge with teacher\\n      \\n\\n      {judge && (\\n        <div className=\"rounded border p-3 text-sm\">\\n          <div className=\"font-medium mb-1\">Teacher auto-judge\\n          <pre className=\"whitespace-pre-wrap overflow-auto\">{JSON.stringify(judge, null, 2)}\\n          <div className=\"text-xs text-gray-600\">Auto-judge is advisory. Human votes remain the source of truth.\\n        \\n      )}\\n\\n      \\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.5 Identity masking policy\\n- Mask model names until vote recorded to reduce brand bias.\\n- Allow optional reveal toggle after vote for learning.\\n- In aggregated views, attribute providers and disclose that models were accessed via Poe.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 5.6 Manual check\\n1. Enter a clearly unsafe prompt and confirm the moderation endpoint blocks the duel.\\n2. Run a normal duel, then click Auto-judge. Confirm a JSON rubric renders and the verdict often aligns with your vote.\\n3. Verify identities remain hidden until you vote, then reveal.\\n4. Confirm Attribution footer appears on Arena and Leaderboard pages.”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Do not rely solely on auto-judge. Treat it as advisory. Keep human votes as the source of truth.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “If local moderation is overly strict or lenient, enable useTeacher: true for borderline cases to reduce false positives and negatives.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “Never log raw prompts or completions in telemetry or logs. Use hashes only.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Identity masking is essential to reduce brand bias. Enforce masking at the UI until a vote is submitted.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Add a small link to your rater guidance in the Leaderboard page so readers understand how votes were collected.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOXvAid3DaM"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 6,\n",
        "“title”: “Packaging, Rate Limits, Tests, and Learn Assessments”,\n",
        "“estimated_tokens”: 2600,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 6. Packaging, resiliency, tests, and assessments\\nThis section finalizes the tutorial by adding rate limit backoff, a POST streaming variant, build tooling, unit tests, and an interactive Learn page with MCQs. You will also wire minimal Tailwind and path aliases so the app runs cleanly with lint and type checks.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.1 Path aliases and Tailwind setup\\nAdd a path alias for @/* and minimal Tailwind so prior pages render as shown. Ensure .env* is ignored.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// tsconfig.json\\n{\\n  \"compilerOptions\": {\\n    \"target\": \"ES2022\",\\n    \"lib\": [\"ES2022\", \"DOM\"],\\n    \"module\": \"ESNext\",\\n    \"moduleResolution\": \"Bundler\",\\n    \"jsx\": \"preserve\",\\n    \"baseUrl\": \".\",\\n    \"paths\": { \"@/\": [\"./\"] },\\n    \"strict\": true,\\n    \"noEmit\": true,\\n    \"allowJs\": false,\\n    \"types\": [\"node\"]\\n  },\\n  \"include\": [\"next-env.d.ts\", \"/*.ts\", \"/.tsx\"],\\n  \"exclude\": [\"node_modules\"]\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// tailwind.config.ts\\nimport type { Config } from \"tailwindcss\";\\n\\nexport default {\\n  content: [\"./app/**/.{ts,tsx}\", \"./components//*.{ts,tsx}\", \"./lib//.{ts,tsx}\"],\\n  theme: { extend: {} },\\n  plugins: [],\\n} satisfies Config;\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// postcss.config.js\\nmodule.exports = { plugins: { tailwindcss: {}, autoprefixer: {} } };\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/globals.css\\n@tailwind base;\\n@tailwind components;\\n@tailwind utilities;\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/layout.tsx\\nimport \"./globals.css\";\\nimport type { Metadata } from \"next\";\\n\\nexport const metadata: Metadata = { title: \"ALAIN Arena on Poe\" };\\n\\nexport default function RootLayout({ children }: { children: React.ReactNode }) {\\n  return (\\n    <html lang=\"en\">\\n      {children}\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “# .gitignore\\nnode_modules\\n.next\\n.env\\n.env.\\n.DS_Store\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.2 Backoff and leaky bucket\\nImplement a tiny per model limiter and exponential backoff with jitter for server routes that call Poe. Apply it to the duel route.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/ratelimit.ts\\nconst buckets = new Map<string, { tokens: number; last: number }>();\\n\\nexport function leakyBucket(key: string, ratePerSec = 5, burst = 10) {\\n  const now = Date.now();\\n  const b = buckets.get(key) || { tokens: burst, last: now };\\n  const elapsed = Math.max(0, now - b.last) / 1000;\\n  b.tokens = Math.min(burst, b.tokens + elapsed * ratePerSec);\\n  b.last = now;\\n  if (b.tokens >= 1) {\\n    b.tokens -= 1;\\n    buckets.set(key, b);\\n    return true;\\n  }\\n  buckets.set(key, b);\\n  return false;\\n}\\n\\nexport async function backoff(fn: () => Promise, tries = 5) {\\n  let attempt = 0;\\n  while (true) {\\n    try {\\n      return await fn();\\n    } catch (e: any) {\\n      attempt++;\\n      if (attempt >= tries) throw e;\\n      const retryAfter = Number(e?.response?.headers?.get?.(\"retry-after\")) || 0;\\n      const base = retryAfter > 0 ? retryAfter * 1000 : 300 * Math.pow(2, attempt - 1);\\n      const jitter = Math.random() * 200;\\n      await new Promise(r => setTimeout(r, base + jitter));\\n    }\\n  }\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.3 POST streaming variant for long prompts\\nEventSource only supports GET. Add a POST variant that streams NDJSON. The client can switch to fetch plus reader for very long prompts.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/duel/route.ts (add POST NDJSON streaming)\\nimport { NextRequest } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { isAllowedModel } from \"@/lib/models\";\\nimport { leakyBucket, backoff } from \"@/lib/ratelimit\";\\nimport crypto from \"crypto\";\\n\\nexport async function POST(req: NextRequest) {\\n  if (!leakyBucket(\"duel_post\", 3, 6)) {\\n    return new Response(JSON.stringify({ error: \"rate_limited\" }), { status: 429 });\\n  }\\n  const body = await req.json().catch(() => ({}));\\n  const { left = \"\", right = \"\", prompt = \"\", temperature = 0.2, top_p = 1, max_tokens = 512 } = body;\\n  if (!isAllowedModel(left) || !isAllowedModel(right)) return new Response(JSON.stringify({ error: \"Unknown model\" }), { status: 400 });\\n  const messages = [{ role: \"user\" as const, content: String(prompt) }];\\n  const promptHash = crypto.createHash(\"sha256\").update(String(prompt)).digest(\"hex\").slice(0, 16);\\n\\n  const stream = new ReadableStream({\\n    async start(controller) {\\n      const enc = new TextEncoder();\\n      function send(event: string, data: any) {\\n        controller.enqueue(enc.encode(JSON.stringify({ event, …data }) + \"\\n\"));\\n      }\\n      const summary: any = { promptHash, params: { temperature, top_p, max_tokens }, left: { ttfbMs: null, totalMs: null }, right: { ttfbMs: null, totalMs: null } };\\n\\n      async function runSide(side: \"left\" | \"right\", model: string) {\\n        const start = Date.now();\\n        let first: number | null = null;\\n        try {\\n          const s: any = await backoff(() => poe.chat.completions.create({ model, messages, temperature, top_p, max_tokens, stream: true }));\\n          for await (const part of s) {\\n            const delta = part?.choices?.[0]?.delta?.content ?? \"\";\\n            if (delta) {\\n              if (first === null) first = Date.now();\\n              send(\"chunk\", { side, delta });\\n            }\\n          }\\n          let usage: any = undefined;\\n          try { const final = await s.finalChatCompletion?.(); usage = final?.usage; } catch {}\\n          summary[side] = { ttfbMs: first ? first - start : null, totalMs: Date.now() - start, usage };\\n        } catch (e: any) {\\n          send(\"error\", { side, message: String(e?.message || e) });\\n          summary[side] = { ttfbMs: null, totalMs: Date.now() - start, error: String(e?.message || e) };\\n        }\\n      }\\n\\n      await Promise.all([runSide(\"left\", left), runSide(\"right\", right)]);\\n      send(\"done\", summary);\\n      controller.close();\\n    }\\n  });\\n\\n  return new Response(stream, { headers: { \"Content-Type\": \"application/x-ndjson\", \"Cache-Control\": \"no-cache\" } });\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.4 Client helper for NDJSON streaming (optional)\\nUse this helper if you switch the Arena to POST when prompts exceed URL limits.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/ndjson.ts\\nexport async function* readNDJSON(res: Response) {\\n  if (!res.body) throw new Error(\"no_body\");\\n  const reader = res.body.getReader();\\n  const dec = new TextDecoder();\\n  let buf = \"\";\\n  while (true) {\\n    const { value, done } = await reader.read();\\n    if (done) break;\\n    buf += dec.decode(value, { stream: true });\\n    let idx;\\n    while ((idx = buf.indexOf(\"\\n\")) >= 0) {\\n      const line = buf.slice(0, idx).trim();\\n      buf = buf.slice(idx + 1);\\n      if (!line) continue;\\n      try { yield JSON.parse(line); } catch {}\\n    }\\n  }\\n  if (buf.trim()) { try { yield JSON.parse(buf.trim()); } catch {} }\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.5 Lint, typecheck, and tests\\nAdd ESLint, Vitest, and minimal tests for Elo and model guard. Wire scripts so CI can run quickly.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// package.json (scripts excerpt)\\n{\\n  \"scripts\": {\\n    \"dev\": \"next dev\",\\n    \"build\": \"next build\",\\n    \"start\": \"next start\",\\n    \"typecheck\": \"tsc –noEmit\",\\n    \"lint\": \"next lint\",\\n    \"test\": \"vitest run\"\\n  }\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// vitest.config.ts\\nimport { defineConfig } from \"vitest/config\";\\nexport default defineConfig({ test: { environment: \"node\", include: [\"tests/**/*.test.ts\"] } });\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// tests/elo.test.ts\\nimport { describe, it, expect } from \"vitest\";\\nimport { updateElo, expectedScore } from \"@/lib/elo\";\\n\\ndescribe(\"Elo\", () => {\\n  it(\"expectedScore symmetric at equal ratings\", () => {\\n    const e = expectedScore(1500, 1500);\\n    expect(Math.abs(e - 0.5)).toBeLessThan(1e-9);\\n  });\\n  it(\"left win increases left rating\", () => {\\n    const [na, nb] = updateElo(1500, 1500, \"LEFT\", 24);\\n    expect(na).toBeGreaterThan(1500);\\n    expect(nb).toBeLessThan(1500);\\n  });\\n  it(\"tie leaves both near baseline\", () => {\\n    const [na, nb] = updateElo(1500, 1500, \"TIE\", 24);\\n    expect(na).toBe(1500);\\n    expect(nb).toBe(1500);\\n  });\\n});\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// tests/models.test.ts\\nimport { describe, it, expect } from \"vitest\";\\nimport { isAllowedModel, COMPARISON_MODELS } from \"@/lib/models\";\\n\\ndescribe(\"models guard\", () => {\\n  it(\"accepts known models\", () => {\\n    for (const m of COMPARISON_MODELS) expect(isAllowedModel(m)).toBe(true);\\n  });\\n  it(\"rejects unknown model\", () => {\\n    expect(isAllowedModel(\"Totally-Not-A-Model\")).toBe(false);\\n  });\\n});\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.6 Learn page with MCQs\\nEmbed the four MCQs from the design so learners can self check. Show rationales after selection.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/learn/page.tsx\\n\"use client\";\\nimport { useState } from \"react\";\\n\\ntype Q = { q: string; opts: string[]; ans: number; why: string };\\nconst qs: Q[] = [\\n  {\\n    q: \"Which header authenticates with Poe?\",\\n    opts: [\"X-Auth-Key\", \"Authorization: Bearer \", \"Poe-Token\", \"Api-Secret\"],\\n    ans: 1,\\n    why: \"Poe follows OpenAI-compatible headers.\",\\n  },\\n  {\\n    q: \"What differs in Poe streaming vs OpenAI streaming?\",\\n    opts: [\"No [DONE] token\", \"Different event name\", \"Identical semantics; some clients drop final event\", \"Binary data\"],\\n    ans: 2,\\n    why: \"SSE semantics match; guard parser for final chunk.\",\\n  },\\n  {\\n    q: \"In Bradley Terry, the gradient term equals what?\",\\n    opts: [\"Refusal rate\", \"Observed minus expected outcome\", \"Token length\", \"Latency variance\"],\\n    ans: 1,\\n    why: \"Update uses y minus pij.\",\\n  },\\n  {\\n    q: \"Why anonymize models during voting?\",\\n    opts: [\"Prevent API overload\", \"Reduce brand bias\", \"Save tokens\", \"Simplify telemetry\"],\\n    ans: 1,\\n    why: \"Masking reduces brand based preferences.\",\\n  },\\n];\\n\\nfunction MCQ({ q, idx, onAnswer }: { q: Q; idx: number; onAnswer: (i: number) => void }) {\\n  const [picked, setPicked] = useState<number | null>(null);\\n  return (\\n    <div className=\"rounded border p-3 space-y-2\">\\n      <div className=\"font-medium\">{idx + 1}. {q.q}\\n      <div className=\"grid gap-2\">\\n        {q.opts.map((opt, i) => (\\n          <button key={i} onClick={() => { setPicked(i); onAnswer(i); }} className=\"text-left rounded border px-3 py-2\">\\n            {String.fromCharCode(65 + i)}. {opt}\\n          \\n        ))}\\n      \\n      {picked !== null && (\\n        <div className=\"text-sm\">\\n          {picked === q.ans ? \"Correct.\" : \"Incorrect.\"} {q.why}\\n        \\n      )}\\n    \\n  );\\n}\\n\\nexport default function LearnPage() {\\n  const [score, setScore] = useState(0);\\n  function onAnswer(i: number, q: Q) { if (i === q.ans) setScore(s => s + 1); }\\n  return (\\n    <main className=\"mx-auto max-w-3xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Learn checks\\n      <div className=\"text-sm text-gray-600\">Click an option to see correctness and rationale. Score increments on first correct click per question.\\n      {qs.map((q, idx) => <MCQ key={idx} q={q} idx={idx} onAnswer={(i) => onAnswer(i, q)} />)}\\n      <div className=\"text-sm\">Score: {score} of {qs.length}\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.7 QA checklist and run commands\\n- npm run typecheck should pass\\n- npm run lint should pass\\n- npm run test should pass Elo and model guard tests\\n- Manual streaming checks\\n  1. GET EventSource duel on /arena\\n  2. POST NDJSON duel with a long prompt via a simple curl or client helper\\n  3. Ratings update on vote\\n  4. Telemetry shows non null averages on /telemetry\\n  5. Leaderboard renders and CI computes\\n  6. Learn page shows MCQs with rationales”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 6.8 Deployment notes\\n- Keep POE_API_KEY in server env only. Do not expose it to the client.\\n- Set NEXT_PUBLIC_* values for defaults and roster only. These have no secrets.\\n- Consider adding an edge cache for static pages; API routes must remain server side due to secrets.\\n- Add a footer with attribution and safety notes on Arena and Leaderboard.”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Switch the Arena page to POST NDJSON when prompts are large. Keep GET EventSource for quick local tests.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Do not run heavy bootstrap confidence intervals on every render. Trigger on demand to avoid UI stalls.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “Leaky bucket and backoff are in memory. For multi instance deploys, move rate limiting to a shared layer or gateway.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Keep a tiny seed set of prompts across domains to avoid domain skew when demoing the leaderboard.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWWEvGF_4QUb"
      },
      "outputs": [],
      "source": [
        "{\n",
        "“section_number”: 7,\n",
        "“title”: “Publishing, Legal, Probes, and Troubleshooting”,\n",
        "“estimated_tokens”: 2400,\n",
        "“content”: [\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “# 7. Publishing, legal, probes, and troubleshooting\\nFinalize your Arena build for real-world use. You will add a probe endpoint to sanity-check models on Poe, an admin page to run probes and visualize health, a legal page with attribution and disclosures, and a small error normalizer to improve operator UX.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.1 Goals\\n- Validate model availability and basic performance before demos\\n- Provide operator-facing health signals without exposing secrets\\n- Publish clear attribution and legal notes\\n- Offer concise troubleshooting with friendlier error messages”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.2 Probe API\\nA minimal probe that measures small-prompt streaming TTFB and total time per model, plus token usage. It runs sequentially to reduce rate-limit risk.\\n\\nPOST /api/probe\\n\\nBody: { models?: string[], prompt?: string, max_tokens?: number }\\n\\nReturns: { results: { model, ok, ttfbMs, totalMs, usage?, error? }[] }”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/api/probe/route.ts\\nimport { NextRequest, NextResponse } from \"next/server\";\\nimport { poe } from \"@/lib/poe\";\\nimport { COMPARISON_MODELS, isAllowedModel } from \"@/lib/models\";\\n\\nexport const runtime = \"nodejs\";\\n\\nexport async function POST(req: NextRequest) {\\n  const body = await req.json().catch(() => ({}));\\n  const models: string[] = Array.isArray(body?.models) && body.models.length\\n    ? body.models.filter((m: string) => isAllowedModel(m))\\n    : […COMPARISON_MODELS];\\n  const prompt: string = body?.prompt || \"Reply with the word: ok\";\\n  const max_tokens: number = Number(body?.max_tokens ?? 32);\\n\\n  const results: any[] = [];\\n  for (const model of models) {\\n    const start = Date.now();\\n    let ttfb: number | null = null;\\n    try {\\n      const stream: any = await poe.chat.completions.create({\\n        model,\\n        stream: true,\\n        max_tokens,\\n        temperature: 0,\\n        messages: [{ role: \"user\", content: prompt }],\\n      });\\n      for await (const part of stream) {\\n        const delta = part?.choices?.[0]?.delta?.content ?? \"\";\\n        if (delta && ttfb === null) ttfb = Date.now() - start;\\n      }\\n      let usage: any = undefined;\\n      try { const final = await stream.finalChatCompletion?.(); usage = final?.usage; } catch {}\\n      results.push({ model, ok: true, ttfbMs: ttfb, totalMs: Date.now() - start, usage });\\n    } catch (e: any) {\\n      results.push({ model, ok: false, ttfbMs: ttfb, totalMs: Date.now() - start, error: String(e?.message || e) });\\n    }\\n  }\\n  return NextResponse.json({ results });\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.3 Admin page to run probes\\nOperators can run a quick health check, see TTFB and totals, and copy JSON for bug reports.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/admin/page.tsx\\n\"use client\";\\nimport { useState } from \"react\";\\nimport { COMPARISON_MODELS } from \"@/lib/models\";\\n\\nfunction Badge({ ok }: { ok: boolean }) {\\n  return <span className={inline-flex items-center rounded px-2 py-0.5 text-xs ${ok ? \\\"bg-green-100 text-green-800\\\" : \\\"bg-red-100 text-red-800\\\"}}>{ok ? \"OK\" : \"ERR\"};\\n}\\n\\nexport default function AdminPage() {\\n  const [running, setRunning] = useState(false);\\n  const [data, setData] = useState(null);\\n  const [prompt, setPrompt] = useState(\"Reply with the word: ok\");\\n\\n  async function run() {\\n    setRunning(true); setData(null);\\n    const res = await fetch(\"/api/probe\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ models: COMPARISON_MODELS, prompt }) });\\n    const json = await res.json();\\n    setData(json);\\n    setRunning(false);\\n  }\\n\\n  return (\\n    <main className=\"mx-auto max-w-4xl p-6 space-y-4\">\\n      <h1 className=\"text-2xl font-semibold\">Admin · Model probe\\n      <div className=\"space-y-2\">\\n        <label className=\"block text-sm\">Probe prompt\\n        <input value={prompt} onChange={e => setPrompt(e.target.value)} className=\"w-full border rounded p-2\" />\\n      \\n      <button onClick={run} disabled={running} className=\"rounded px-4 py-2 bg-black text-white\">{running ? \"Probing…\" : \"Run probe\"}\\n\\n      {data?.results && (\\n        <div className=\"overflow-auto\">\\n          <table className=\"w-full text-sm border-collapse mt-4\">\\n            \\n              <tr className=\"text-left border-b\">\\n                <th className=\"py-2\">Model\\n                <th className=\"py-2\">Status\\n                <th className=\"py-2\">TTFB ms\\n                <th className=\"py-2\">Total ms\\n                <th className=\"py-2\">Prompt tok\\n                <th className=\"py-2\">Completion tok\\n                <th className=\"py-2\">Error\\n              \\n            \\n            \\n              {data.results.map((r: any) => (\\n                <tr key={r.model} className=\"border-b\">\\n                  <td className=\"py-2\">{r.model}\\n                  <td className=\"py-2\">\\n                  <td className=\"py-2\">{r.ttfbMs ?? \"\"}\\n                  <td className=\"py-2\">{r.totalMs ?? \"\"}\\n                  <td className=\"py-2\">{r.usage?.prompt_tokens ?? \"\"}\\n                  <td className=\"py-2\">{r.usage?.completion_tokens ?? \"\"}\\n                  <td className=\"py-2 truncate max-w-[240px]\" title={r.error || \"\"}>{r.error || \"\"}\\n                \\n              ))}\\n            \\n          \\n          <details className=\"rounded border p-3 text-sm mt-3\">\\n            Raw JSON\\n            <pre className=\"overflow-auto whitespace-pre-wrap\">{JSON.stringify(data, null, 2)}\\n          \\n        \\n      )}\\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.4 Legal and attribution page\\nPublish disclosures so testers understand that models are accessed via Poe and capabilities may differ from vendor-native APIs. Add this static page and link it from your footer.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// app/legal/page.tsx\\nexport default function LegalPage() {\\n  return (\\n    <main className=\"mx-auto max-w-3xl p-6 space-y-4 text-sm\">\\n      <h1 className=\"text-2xl font-semibold\">Legal, attribution, and disclosures\\n      <section className=\"space-y-2\">\\n        This demo accesses public bots via Poe’s OpenAI-compatible API. Capabilities, limits, and costs may differ from vendor-direct endpoints. Model names and provider trademarks belong to their respective owners.\\n        <ul className=\"list-disc ml-5\">\\n          We do not store raw prompts or completions. We record prompt hashes and aggregate telemetry only.\\n          Votes are anonymized and identities are revealed only after vote submission.\\n          Ratings reflect pairwise preferences collected here and are not absolute measures of model quality.\\n          Use results responsibly and avoid over-generalizing from small samples.\\n        \\n        See Poe terms and provider usage policies before production use.\\n      \\n    \\n  );\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.5 Error normalization helper\\nMap common upstream failures to concise messages for admin and rater views. Use this when reporting errors in the UI.”\n",
        "},\n",
        "{\n",
        "“cell_type”: “code”,\n",
        "“source”: “// lib/errors.ts\\nexport function normalizeError(message: string | undefined) {\\n  const m = (message || \"\").toLowerCase();\\n  if (m.includes(\"rate\") && m.includes(\"limit\")) return \"Rate limited. Please retry in a moment.\";\\n  if (m.includes(\"timeout\")) return \"Upstream timeout. Try a shorter prompt or reduce traffic.\";\\n  if (m.includes(\"insufficient\") || m.includes(\"payment\") || m.includes(\"credits\")) return \"Insufficient credits on Poe. Check your subscription.\";\\n  if (m.includes(\"authorization\") || m.includes(\"unauthorized\") || m.includes(\"api key\")) return \"Auth error. Server is missing or has an invalid Poe API key.\";\\n  if (m.includes(\"too large\") || m.includes(\"max tokens\") || m.includes(\"token limit\")) return \"Prompt too large for this model on Poe. Reduce length or use fewer examples.\";\\n  return message || \"Unknown error\";\\n}\\n”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.6 Troubleshooting checklist\\n- Smoke test /api/health then /api/smoke\\n- Run /admin probe and confirm at least two models stream\\n- If GET EventSource fails on long prompts, switch Arena to POST NDJSON\\n- If votes do not update ratings, check Zustand persistence and ensure record() is called\\n- If telemetry is empty, verify the done event path calls addTelem()\\n- If styles look off, confirm Tailwind is configured and globals.css loaded\\n- If imports fail, verify tsconfig paths and baseUrl”\n",
        "},\n",
        "{\n",
        "“cell_type”: “markdown”,\n",
        "“source”: “## 7.7 Pre-publish operator runbook\\n1. Probe the full roster on /admin and snapshot JSON\\n2. Run three duels per pair you plan to demo and confirm votes record\\n3. Check /leaderboard and /telemetry for plausible stats\\n4. Review /legal for disclosures and add your organization links if needed\\n5. Keep the teacher model in rotation for one of every three duels during live demos”\n",
        "}\n",
        "],\n",
        "“callouts”: [\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Run probes sequentially to reduce burstiness against Poe. Keep the prompt trivial like “ok” to limit token spend.”\n",
        "},\n",
        "{\n",
        "“type”: “Warning”,\n",
        "“message”: “Do not imply vendor endorsement. Clearly state that results reflect your environment and may not match vendor-native APIs.”\n",
        "},\n",
        "{\n",
        "“type”: “Note”,\n",
        "“message”: “If you plan to publish public leaderboards, add consent language for raters and a data-retention policy.”\n",
        "},\n",
        "{\n",
        "“type”: “Tip”,\n",
        "“message”: “Surface normalizeError() messages in the Arena UI to help testers self-serve common failures.”\n",
        "}\n",
        "]\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
