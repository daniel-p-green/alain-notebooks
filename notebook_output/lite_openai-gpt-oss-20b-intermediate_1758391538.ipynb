{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d3e050",
   "metadata": {},
   "source": [
    "# Lite Notebook · openai/gpt-oss-20b · Intermediate\n",
    "\n",
    "Token‑light tutorial: environment setup + runnable calls. Uses OpenAI SDK against selected provider (Poe/OpenAI‑compatible/local).\n",
    "\n",
    "Details:\n",
    "- Provider: poe\n",
    "- Model: gpt-oss-20b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada59bb",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Configure provider and API key correctly\n",
    "- Run a model call with safe defaults\n",
    "- Tune basic parameters and/or streaming\n",
    "- Record simple telemetry or ranking step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8b1c0",
   "metadata": {},
   "source": [
    "## Step 1: Streaming Basics\n",
    "\n",
    "Use streaming to improve perceived latency, but start by verifying your Poe environment variables map into the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df589074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "POE_KEY = os.getenv(\"POE_API_KEY\")\n",
    "if not os.getenv(\"OPENAI_API_KEY\") and POE_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = POE_KEY\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set POE_API_KEY (preferred) or OPENAI_API_KEY before running this notebook.\")\n",
    "\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\") or \"https://api.poe.com/v1\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = base_url\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "explanation = dedent(\"\"\"\n",
    "Streaming returns tokens incrementally so people can read along.\n",
    "Non-streaming waits for the whole message, which is simpler for batch jobs\n",
    "or when you need atomic JSON payloads. Use streaming for interactive UIs,\n",
    "and non-streaming for deterministic post-processing pipelines.\n",
    "\"\"\").strip()\n",
    "\n",
    "print(f\"OPENAI_BASE_URL -> {base_url}\")\n",
    "print(explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11a0b5",
   "metadata": {},
   "source": [
    "## Step 2: Streaming Demo\n",
    "\n",
    "Stream the model’s response for the brief and capture the incremental chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You help developers prototype streaming chat flows.\"},\n",
    "    {\"role\": \"user\", \"content\": \"introduction to harmony prompt format\"},\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-oss-20b\",\n",
    "    messages=messages,\n",
    "    temperature=0.5,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "stream_chunks: List[str] = []\n",
    "print(\"Streaming reply:\\n\")\n",
    "for chunk in stream:\n",
    "    choice = chunk.choices[0]\n",
    "    delta = getattr(choice, \"delta\", None) or {}\n",
    "    text = delta.get(\"content\")\n",
    "    if text:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        stream_chunks.append(text)\n",
    "\n",
    "print()\n",
    "streamed_reply = \"\".join(stream_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d12197",
   "metadata": {},
   "source": [
    "## Step 3: Telemetry\n",
    "\n",
    "Capture latency and token usage from the response. Re-use the streaming summary to gather quick telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "telemetry_prompt = (\n",
    "    \"Summarize the streaming walkthrough in four concise bullet points. \"\n",
    "    \"Keep the answer under 120 tokens.\"\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "telemetry_response = client.chat.completions.create(\n",
    "    model=\"gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise telemetry aide.\"},\n",
    "        {\"role\": \"user\", \"content\": telemetry_prompt},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    ")\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "completion_message = telemetry_response.choices[0].message\n",
    "print(completion_message.get(\"content\", \"(no content returned)\"))\n",
    "\n",
    "usage = getattr(telemetry_response, \"usage\", None)\n",
    "if usage:\n",
    "    print(\n",
    "        f\"Latency: {elapsed:.2f}s | prompt_tokens={usage.prompt_tokens} \"\n",
    "        f\"completion_tokens={usage.completion_tokens} total_tokens={usage.total_tokens}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Latency: {elapsed:.2f}s (token usage unavailable)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370b700",
   "metadata": {},
   "source": [
    "Note: Live API calls require OPENAI_API_KEY and OPENAI_BASE_URL to be set. The setup cell helps map keys for Poe or gateways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
