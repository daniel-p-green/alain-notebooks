{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-p-green/alain-notebooks/blob/main/FutureAGI_Agent_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32727d13",
      "metadata": {
        "id": "32727d13"
      },
      "source": [
        "# FutureAGI Agent Optimizer ğŸš€ â€” Interactive Demo\n",
        "\n",
        "This notebook demonstrates how to use our `agent-opt` library to automatically improve and optimize LLM agents and prompts.\n",
        "It runs a small Question-Answering optimization across multiple optimizers and compares the best prompts found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6376df67",
      "metadata": {
        "id": "6376df67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d8de8013-8a1f-4138-abe6-5b7b7f241aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Installation\n",
        "# Install dependencies\n",
        "!pip install agent-opt -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title API Key Setup\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Enter your API keys interactively (Jupyter will prompt)\n",
        "OPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY: ')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print(\"âœ… API Key set successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "NxJEZ8PQE8JU",
        "outputId": "17058148-1fed-4e71-fe7b-b9db9779edc5"
      },
      "id": "NxJEZ8PQE8JU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OPENAI_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… API Key set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0dcc117",
      "metadata": {
        "id": "b0dcc117"
      },
      "source": [
        "## Imports and Logging Configuration\n",
        "\n",
        "Import the required modules and configure logging so you can follow optimizer progress.  \n",
        "If an import fails, make sure the corresponding package is installed in your kernel environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95adb4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "d95adb4a",
        "outputId": "e5dc8285-9018-46ee-c491-8e4fb3ec59f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All components imported and logging is configured.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# CELL 2 â€” Imports and logging configuration\n",
        "import logging\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Framework Imports ---\n",
        "from fi.opt.base.base_optimizer import BaseOptimizer\n",
        "from fi.opt.generators import LiteLLMGenerator\n",
        "from fi.opt.datamappers import BasicDataMapper\n",
        "from fi.opt.base.evaluator import Evaluator\n",
        "from fi.opt.types import OptimizationResult, IterationHistory\n",
        "from fi.opt.utils import setup_logging\n",
        "\n",
        "# --- Evaluator Imports ---\n",
        "from fi.evals.metrics import CustomLLMJudge\n",
        "from fi.evals.llm import LiteLLMProvider\n",
        "\n",
        "# --- Import All Optimizers for on the fly changing ---\n",
        "from fi.opt.optimizers import (\n",
        "    RandomSearchOptimizer,\n",
        "    ProTeGi,\n",
        "    MetaPromptOptimizer,\n",
        "    GEPAOptimizer,\n",
        "    PromptWizardOptimizer,\n",
        "    BayesianSearchOptimizer,\n",
        ")\n",
        "\n",
        "# Configure logging\n",
        "setup_logging(level=logging.INFO, log_to_console=True, log_to_file=True, log_file=\"agent-opt.log\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… All components imported and logging is configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e298570f",
      "metadata": {
        "id": "e298570f"
      },
      "source": [
        "## Prepare the Dataset\n",
        "\n",
        "Create an in-memory QA dataset. Replace with your CSV/JSON loader for a real experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b42715",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6b42715",
        "outputId": "88ef60cc-d250-44bc-a039-1b364b69cc47",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Sample dataset created successfully. Here are the first two examples:\n",
            "{'context': 'The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.', 'question': 'Who designed the Eiffel Tower?', 'answer': 'Gustave Eiffel'}\n",
            "{'context': 'Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that converts carbon dioxide and water into glucose and oxygen.', 'question': 'What are the products of photosynthesis?', 'answer': 'Glucose and oxygen'}\n"
          ]
        }
      ],
      "source": [
        "# @title Create sample dataset\n",
        "def create_dataset() -> List[Dict[str, Any]]:\n",
        "    '''Creates a sample dataset for the QA task.'''\n",
        "    data = {\n",
        "        'context': [\n",
        "            \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
        "            \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that converts carbon dioxide and water into glucose and oxygen.\",\n",
        "            \"The first person to walk on the Moon was Neil Armstrong. The historic event occurred on July 20, 1969, during the Apollo 11 mission.\",\n",
        "            \"The Amazon River in South America is the largest river by discharge volume of water in the world, and the second longest in length.\",\n",
        "            \"William Shakespeare was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language. His plays have been translated into every major living language.\"\n",
        "        ],\n",
        "        'question': [\n",
        "            \"Who designed the Eiffel Tower?\",\n",
        "            \"What are the products of photosynthesis?\",\n",
        "            \"When did a person first walk on the Moon?\",\n",
        "            \"Which river is the largest by water volume?\",\n",
        "            \"What is Shakespeare known for?\"\n",
        "        ],\n",
        "        'answer': [\n",
        "            \"Gustave Eiffel\",\n",
        "            \"Glucose and oxygen\",\n",
        "            \"July 20, 1969\",\n",
        "            \"The Amazon River\",\n",
        "            \"Being the greatest writer in the English language\"\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df.to_dict(\"records\")\n",
        "\n",
        "dataset = create_dataset()\n",
        "print(\"âœ… Sample dataset created successfully. Here are the first two examples:\")\n",
        "for item in dataset[:2]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d67b2ee",
      "metadata": {
        "id": "1d67b2ee"
      },
      "source": [
        "## Define the Evaluation Strategy\n",
        "\n",
        "We use a Custom LLM-as-a-Judge to score responses from 0.0 to 1.0. You can replace this with a local metric for cost savings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6f8266",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6f8266",
        "outputId": "7391f4fd-92d7-4152-f9fd-8280a60d8e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 09:51:50,785 - fi.opt.base.evaluator - INFO - Initialized Evaluator with local metric: CustomLLMJudge\n",
            "âœ… Evaluation strategy defined using a Custom LLM-as-a-Judge.\n"
          ]
        }
      ],
      "source": [
        "# LLM provider used by the judge\n",
        "provider = LiteLLMProvider()\n",
        "\n",
        "correctness_judge_config = {\n",
        "    \"name\": \"correctness_judge\",\n",
        "    \"grading_criteria\": '''You are evaluating an AI's answer to a question. The score must be 1.0 if the 'response'\n",
        "is semantically equivalent to the 'expected_response' (the ground truth). The score should be 0.0 if it is incorrect.\n",
        "Partial credit is acceptable. For example, if the expected answer is \"Gustave Eiffel\" and the response is\n",
        "\"The tower was designed by Eiffel\", a score of 0.8 is appropriate.''',\n",
        "}\n",
        "\n",
        "# Instantiate the judge and evaluator wrapper\n",
        "correctness_judge = CustomLLMJudge(provider, config=correctness_judge_config)\n",
        "evaluator = Evaluator(metric=correctness_judge)\n",
        "\n",
        "# Data mapper connects model outputs to the judge expectations\n",
        "data_mapper = BasicDataMapper(\n",
        "    key_map={\n",
        "        \"response\": \"generated_output\",\n",
        "        \"expected_response\": \"answer\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"âœ… Evaluation strategy defined using a Custom LLM-as-a-Judge.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd7d895",
      "metadata": {
        "id": "4cd7d895"
      },
      "source": [
        "## Initial Prompt and Models\n",
        "\n",
        "Define the initial prompt and generator model to optimize for, and teacher models to optimize using.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874202bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "874202bc",
        "outputId": "790c7478-3c92-4683-d61e-7dad1bf4377d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ready to optimize! We will improve `Context: {context}\\nQuestion: {question}\\nAnswer:`\n"
          ]
        }
      ],
      "source": [
        "# @title Prompt Setup\n",
        "INITIAL_PROMPT = \"Context: {context}\\\\nQuestion: {question}\\\\nAnswer:\" # @param {\"type\":\"string\"}\n",
        "GENERATOR_MODEL = \"gpt-4o-mini\" # @param {\"type\":\"string\"}\n",
        "TEACHER_MODEL = \"gpt-5\" # @param {\"type\":\"string\"}\n",
        "\n",
        "print(f\"âœ… Ready to optimize! We will improve `{INITIAL_PROMPT}`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEPA Optimizer Setup\n",
        "For this demo, we will be using **GEPA** optimizer.\n",
        "\n",
        " **GEPA** (Genetic-Pareto), is a state-of-the-art evolutionary algorithm for prompt optimization. Instead of making small, random changes, GEPA treats prompts like DNA and intelligently evolves them over generations.\n",
        "\n",
        "### How it Works:\n",
        "1.  **Evaluate:** It first tests the performance of the current best prompt(s) on a sample of data.\n",
        "2.  **Reflect:** It uses a powerful \"reflection\" model (our `reflection_model`) to analyze the results, especially the failures. It generates rich, textual feedback on *why* the prompt failed.\n",
        "3.  **Mutate:** Based on this reflection, it rewrites the prompt to create new, improved \"offspring\" prompts.\n",
        "4.  **Select:** It uses a sophisticated method called Pareto-aware selection to choose the most promising new prompts to carry forward to the next generation. This ensures that it doesn't just find one good prompt, but a diverse set of high-performing ones.\n",
        "\n",
        "This cycle of **Evaluate -> Reflect -> Mutate -> Select** allows GEPA to navigate the vast space of possible prompts much more efficiently than random chance, often leading to significant performance improvements.\n",
        "\n",
        "For more information refer to our [FutureAGI Optimization Docs!](https://docs.futureagi.com/future-agi/get-started/optimization/optimizers/overview)"
      ],
      "metadata": {
        "id": "bhtoZL7jBxk1"
      },
      "id": "bhtoZL7jBxk1"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = GEPAOptimizer(reflection_model=TEACHER_MODEL, generator_model=GENERATOR_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYq-hYJCbk5",
        "outputId": "3eeabaf9-4b29-40af-8589-9ccd2f5babc8"
      },
      "id": "ZLYq-hYJCbk5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 09:52:00,416 - fi.opt.optimizers.gepa - INFO - Initialized with reflection_model: gpt-5, generator_model: gpt-4o-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4c3146",
      "metadata": {
        "id": "cd4c3146"
      },
      "source": [
        "## Run the GEPA Optimizer\n",
        "\n",
        "This cell runs each GEPA Optimizer. It may take time and WILL consume API Credits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559ba6df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "559ba6df",
        "outputId": "8558e0c1-3669-46bd-f171-bb46519bc3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 09:52:03,121 - fi.opt.optimizers.gepa - INFO - --- Starting GEPA Prompt Optimization ---\n",
            "2025-10-10 09:52:03,123 - fi.opt.optimizers.gepa - INFO - Dataset size: 5\n",
            "2025-10-10 09:52:03,123 - fi.opt.optimizers.gepa - INFO - Initial prompts: ['Context: {context}\\\\nQuestion: {question}\\\\nAnswer:']\n",
            "2025-10-10 09:52:03,124 - fi.opt.optimizers.gepa - INFO - Max metric calls: 40\n",
            "2025-10-10 09:52:03,125 - fi.opt.optimizers.gepa - INFO - Creating internal GEPA adapter...\n",
            "2025-10-10 09:52:03,126 - fi.opt.optimizers.gepa - INFO - Initialized with generator_model: gpt-4o-mini\n",
            "2025-10-10 09:52:03,126 - fi.opt.optimizers.gepa - INFO - Seed candidate for GEPA: {'prompt': 'Context: {context}\\\\nQuestion: {question}\\\\nAnswer:'}\n",
            "2025-10-10 09:52:03,127 - fi.opt.optimizers.gepa - INFO - Calling gepa.optimize...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:   0%|          | 0/40 [00:00<?, ?rollouts/s]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 09:52:03,130 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:52:03,131 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-10 09:52:03,131 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-10 09:52:03,132 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:52:07,313 - fi.opt.optimizers.gepa - INFO - Output generation finished in 4.18s.\n",
            "2025-10-10 09:52:07,315 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:52:07,316 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:52:07,317 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-10 09:52:07,319 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:52:14,452 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response accurately identifies Gustave Eiffel as the designer of the Eiffel Tower, though it includes additional information about his company.\"\n",
            "}\n",
            "2025-10-10 09:52:14,454 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately identifies the products of photosynthesis as glucose and oxygen, which matches the expected response semantically.\"\n",
            "}\n",
            "2025-10-10 09:52:14,455 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response provides the correct date when a person first walked on the Moon, which is July 20, 1969. However, the inclusion of additional context (\\\"A person first walked on the Moon\\\") slightly deviates from the expected simple answer format.\"\n",
            "}\n",
            "2025-10-10 09:52:14,455 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 0.3000\n",
            "Reason: {\n",
            "  \"score\": 0.3,\n",
            "  \"reason\": \"The response correctly identifies the Amazon River but adds unnecessary information about its discharge, which is not required by the expected response.\"\n",
            "}\n",
            "2025-10-10 09:52:14,457 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 0.5000\n",
            "Reason: {\n",
            "  \"score\": 0.5,\n",
            "  \"reason\": \"The response recognizes Shakespeare as a renowned figure in English literature and provides detailed elaboration on his works and impact. However, it does not directly state that he is 'the greatest writer in the English language,' which is the key point of the expected response.\"\n",
            "}\n",
            "2025-10-10 09:52:14,458 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-10 09:52:14,459 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 7.14s.\n",
            "2025-10-10 09:52:14,459 - fi.opt.optimizers.gepa - INFO - Scores: [0.8, 1.0, 0.9, 0.3, 0.5]\n",
            "2025-10-10 09:52:14,460 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 11.33s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  12%|â–ˆâ–        | 5/40 [00:11<01:19,  2.27s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Base program full valset score: 0.7\n",
            "Iteration 1: Selected program 0 score: 0.7\n",
            "2025-10-10 09:52:14,463 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:52:14,464 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-10 09:52:14,464 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:52:14,465 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:52:18,325 - fi.opt.optimizers.gepa - INFO - Output generation finished in 3.86s.\n",
            "2025-10-10 09:52:18,327 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:52:18,327 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:52:18,328 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:52:18,329 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:52:23,739 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.5000\n",
            "Reason: {\n",
            "  \"score\": 0.5,\n",
            "  \"reason\": \"The response comprehensively elaborates on Shakespeare's impact and achievements, aligning with the expected notion of him being the greatest writer in English. However, it expands beyond the succinctness of the expected response.\"\n",
            "}\n",
            "2025-10-10 09:52:23,742 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response provides the correct date and additional context, which is accurate. However, it exceeds the expected succinct answer format.\"\n",
            "}\n",
            "2025-10-10 09:52:23,742 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response correctly identifies 'glucose and oxygen' as the products of photosynthesis, matching the expected response. Minor additional detail about them being products doesn't impact semantic equivalence.\"\n",
            "}\n",
            "2025-10-10 09:52:23,743 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:52:23,744 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.42s.\n",
            "2025-10-10 09:52:23,744 - fi.opt.optimizers.gepa - INFO - Scores: [0.5, 0.9, 0.9]\n",
            "2025-10-10 09:52:23,746 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:52:23,747 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 9.28s.\n",
            "2025-10-10 09:52:23,748 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-10 09:52:23,748 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-10 09:52:23,749 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 1: Proposed new text for prompt: You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to output only the answer text, to be placed after \"Answer:\". Follow these rules:\n",
            "\n",
            "- Use only information from the provided Context. Do not use outside knowledge.\n",
            "- Output the shortest accurate phrase, value, or list that directly answers the Question.\n",
            "- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanation, examples, or extra context.\n",
            "- Reuse exact wording from the Context when possible. If needed, minimally condense by removing hedges like â€œwidely regarded asâ€ to keep the core claim.\n",
            "- Preserve the original capitalization, spelling, numbers, and units as they appear in the Context.\n",
            "- If the answer is a date, output it exactly in the format given in the Context (e.g., \"July 20, 1969\").\n",
            "- If the answer is a list, output just the items in the same order and wording as in the Context, joined as they appear (e.g., â€œglucose and oxygenâ€).\n",
            "- For â€œWhat is X known for?â€ prefer the primary accolade or superlative stated in the Context (e.g., â€œbeing the greatest writer in the English languageâ€) over general roles or examples.\n",
            "- Do not include any preamble, postscript, quotations, or punctuation beyond what is intrinsic to the answer itself. Output a single line with no leading or trailing spaces.\n",
            "- If the Context does not contain the answer, output: Not provided in context.\n",
            "\n",
            "Aim for maximal brevity while remaining fully accurate and faithful to the Context.\n",
            "2025-10-10 09:53:28,950 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:53:28,952 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to ou...'\n",
            "2025-10-10 09:53:28,953 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:53:28,953 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:53:31,967 - fi.opt.optimizers.gepa - INFO - Output generation finished in 3.01s.\n",
            "2025-10-10 09:53:31,969 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:53:31,970 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:53:31,971 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:53:31,971 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:53:34,639 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, differing only in capitalization, which does not affect the meaning.\"\n",
            "}\n",
            "2025-10-10 09:53:34,642 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is identical to the expected response, both providing the exact same date: July 20, 1969.\"\n",
            "}\n",
            "2025-10-10 09:53:34,644 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, with only minor capitalization differences, which do not affect the meaning.\"\n",
            "}\n",
            "2025-10-10 09:53:34,644 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:53:34,646 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.67s.\n",
            "2025-10-10 09:53:34,646 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 1.0]\n",
            "2025-10-10 09:53:34,647 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 5.70s.\n",
            "Iteration 1: New subsample score 3.0 is better than old score 2.3. Continue to full eval and add to candidate pool.\n",
            "2025-10-10 09:53:34,647 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:53:34,648 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to ou...'\n",
            "2025-10-10 09:53:34,649 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-10 09:53:34,650 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:53:37,353 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.70s.\n",
            "2025-10-10 09:53:37,354 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:53:37,357 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:53:37,357 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-10 09:53:37,358 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:53:42,919 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:53:42,921 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response correctly identifies 'glucose and oxygen'. The only difference is a capitalization issue, which is minor in this context.\"\n",
            "}\n",
            "2025-10-10 09:53:42,922 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is exactly the same as the expected response.\"\n",
            "}\n",
            "2025-10-10 09:53:42,924 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, with a slight variation in phrasing. Partial credit awarded due to the omission of 'The'.\"\n",
            "}\n",
            "2025-10-10 09:53:42,924 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately matches the expected response except for the capitalization of the initial word, which does not affect the semantic meaning. Therefore, it is semantically equivalent.\"\n",
            "}\n",
            "2025-10-10 09:53:42,926 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-10 09:53:42,927 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.57s.\n",
            "2025-10-10 09:53:42,928 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.9, 1.0, 0.9, 1.0]\n",
            "2025-10-10 09:53:42,929 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 8.28s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [01:39<02:39,  6.65s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: New program is on the linear pareto front\n",
            "Iteration 1: Full valset score for new program: 0.96\n",
            "Iteration 1: Full train_val score for new program: 0.96\n",
            "Iteration 1: Individual valset scores for new program: [1.0, 0.9, 1.0, 0.9, 1.0]\n",
            "Iteration 1: New valset pareto front scores: [1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "Iteration 1: Full valset pareto front score: 0.9800000000000001\n",
            "Iteration 1: Updated valset pareto front programs: [{1}, {0}, {1}, {1}, {1}]\n",
            "Iteration 1: Best valset aggregate score so far: 0.96\n",
            "Iteration 1: Best program as per aggregate score on train_val: 1\n",
            "Iteration 1: Best program as per aggregate score on valset: 1\n",
            "Iteration 1: Best score on valset: 0.96\n",
            "Iteration 1: Best score on train_val: 0.96\n",
            "Iteration 1: Linear pareto front program index: 1\n",
            "Iteration 1: New program candidate index: 1\n",
            "Iteration 2: Selected program 1 score: 0.96\n",
            "2025-10-10 09:53:42,932 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:53:42,933 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to ou...'\n",
            "2025-10-10 09:53:42,934 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:53:42,935 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:53:44,170 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.23s.\n",
            "2025-10-10 09:53:44,172 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:53:44,173 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:53:44,173 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:53:44,174 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:53:47,168 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response provides the correct answer with additional context. It is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:53:47,170 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response and the expected response are semantically equivalent, differing only in capitalization, which does not affect the meaning.\"\n",
            "}\n",
            "2025-10-10 09:53:47,172 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, with only a difference in capitalization.\"\n",
            "}\n",
            "2025-10-10 09:53:47,173 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:53:47,175 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.00s.\n",
            "2025-10-10 09:53:47,176 - fi.opt.optimizers.gepa - INFO - Scores: [0.9, 1.0, 1.0]\n",
            "2025-10-10 09:53:47,177 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:53:47,178 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 4.25s.\n",
            "2025-10-10 09:53:47,179 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-10 09:53:47,181 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-10 09:53:47,181 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 2: Proposed new text for prompt: You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to output only the answer text to place after \"Answer:\". Follow these rules:\n",
            "\n",
            "- Use only information explicitly stated in the Context. Do not use outside knowledge or inference beyond matching synonyms in the question to the Context.\n",
            "- Output the shortest complete phrase, value, or list that directly answers the Question.\n",
            "- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanations, examples, or extra context.\n",
            "- Reuse exact wording from the Context whenever possible. If needed, minimally condense by removing non-essential hedges or descriptors (e.g., â€œwidely regarded asâ€), while preserving the core entity or value.\n",
            "- Do not add words not present in the Contextâ€”especially avoid adding determiners (e.g., â€œtheâ€, â€œaâ€), roles/titles (e.g., â€œengineerâ€, â€œDr.â€), or adjectivesâ€”unless they are part of the exact phrase you copy.\n",
            "- Preserve the original capitalization, spelling, numbers, and units exactly as they appear in the Context for the words you output. If a leading â€œTheâ€ appears as part of the phrase in the Context, keep it with the same casing; otherwise do not add or remove it.\n",
            "- If the answer is a date or time, output it exactly in the format given in the Context.\n",
            "- If the answer is a number, duration, or quantity, output only the numeric value and its unit if present (e.g., â€œ5 kmâ€), with no extra words.\n",
            "- If the answer is a list, output the items in the same wording and order as in the Context, joined exactly as they appear there (keep the same commas, â€œandâ€, slashes, etc.). Do not reorder, add, or omit items.\n",
            "- Resolve entity type by the question:\n",
            "  - Who â†’ output the personâ€™s name only (no titles/roles), unless the question explicitly asks for a title or organization.\n",
            "  - Which/What (entity) â†’ output the entityâ€™s name only.\n",
            "  - Which company/organization â†’ output the organizationâ€™s name only.\n",
            "  - Where â†’ output the minimal location phrase that fully answers the question.\n",
            "  - When â†’ output the date/time exactly as stated.\n",
            "  - How many/much/long â†’ output only the value (and unit, if present).\n",
            "- If multiple mentions exist, select the one that directly satisfies the question. If both a person and their company are credited, prefer:\n",
            "  - â€œWho â€¦?â€ â†’ the personâ€™s name.\n",
            "  - â€œWhich company/organization â€¦?â€ â†’ the company/organization name.\n",
            "- For â€œWhat is X known for?â€ prefer the primary accolade or superlative stated in the Context over general roles or examples, using the exact phrasing.\n",
            "- Output exactly one line with no leading/trailing spaces, no quotation marks, and no added punctuation beyond what is intrinsic to the answer text.\n",
            "- If the Context does not contain the answer, output exactly: Not provided in context.\n",
            "2025-10-10 09:54:49,400 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:54:49,401 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your ...'\n",
            "2025-10-10 09:54:49,402 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:54:49,403 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:54:50,716 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.31s.\n",
            "2025-10-10 09:54:50,718 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:54:50,719 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:54:50,719 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:54:50,720 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:54:55,315 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is identical to the expected response, making it semantically equivalent.\"\n",
            "}\n",
            "2025-10-10 09:54:55,318 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response is very similar to the expected response, only missing the article 'The'.\"\n",
            "}\n",
            "2025-10-10 09:54:55,319 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response accurately identifies the entity as 'Amazon River', omitting only the article 'The', which is a minor discrepancy.\"\n",
            "}\n",
            "2025-10-10 09:54:55,319 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:54:55,320 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.60s.\n",
            "2025-10-10 09:54:55,321 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.9, 0.9]\n",
            "2025-10-10 09:54:55,322 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 5.92s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [02:52<02:35,  8.66s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2: New subsample score 2.8 is not better than old score 2.9, skipping\n",
            "Iteration 3: Selected program 1 score: 0.96\n",
            "2025-10-10 09:54:55,325 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:54:55,325 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to ou...'\n",
            "2025-10-10 09:54:55,326 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:54:55,326 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:54:57,925 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.60s.\n",
            "2025-10-10 09:54:57,927 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:54:57,928 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:54:57,929 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:54:57,930 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:55:00,927 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, only differing in capitalization.\"\n",
            "}\n",
            "2025-10-10 09:55:00,929 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:55:00,930 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, and the only difference is the capitalization which does not impact semantic content.\"\n",
            "}\n",
            "2025-10-10 09:55:00,930 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:55:00,931 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.00s.\n",
            "2025-10-10 09:55:00,933 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 1.0]\n",
            "2025-10-10 09:55:00,934 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:55:00,934 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 5.61s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/40 [02:57<01:48,  7.24s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3: All subsample scores perfect. Skipping.\n",
            "Iteration 3: Reflective mutation did not propose a new candidate\n",
            "Iteration 4: Selected program 0 score: 0.7\n",
            "2025-10-10 09:55:00,935 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:55:00,936 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-10 09:55:00,936 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:55:00,939 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:55:03,040 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.10s.\n",
            "2025-10-10 09:55:03,042 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:55:03,043 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:55:03,044 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:55:03,044 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:55:07,850 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response correctly states the significant detail, the date, but includes additional unnecessary information.\"\n",
            "}\n",
            "2025-10-10 09:55:07,852 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.6000\n",
            "Reason: {\n",
            "  \"score\": 0.6,\n",
            "  \"reason\": \"The response correctly identifies the Amazon River but adds additional information about it being the largest by discharge volume, which, while true, is not required. Partial credit is given for mentioning the correct river.\"\n",
            "}\n",
            "2025-10-10 09:55:07,853 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response includes the key information 'Gustave Eiffel' but adds additional, unnecessary details. It is semantically equivalent but not precise. Thus, partial credit of 0.8 is appropriate.\"\n",
            "}\n",
            "2025-10-10 09:55:07,854 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:55:07,854 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.81s.\n",
            "2025-10-10 09:55:07,855 - fi.opt.optimizers.gepa - INFO - Scores: [0.9, 0.6, 0.8]\n",
            "2025-10-10 09:55:07,856 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:55:07,856 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.92s.\n",
            "2025-10-10 09:55:07,857 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-10 09:55:07,858 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-10 09:55:07,858 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 4: Proposed new text for prompt: You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task: Fill in the Answer line with the single, minimal phrase from the Context that directly answers the Question.\n",
            "\n",
            "Rules:\n",
            "- Extractive only: copy the answer text exactly as it appears in the Context (preserve wording, capitalization, punctuation, units, and date formats).\n",
            "- Be minimal: use the fewest words that fully answer the question (e.g., a name, date, number, or noun phrase).\n",
            "- No sentences: do not restate the question or form a full sentence. Do not add explanations, qualifiers, or extra facts.\n",
            "- No extras: do not add articles or descriptors not present in the minimal answer span. Do not append additional details (e.g., missions, roles, definitions).\n",
            "- Answer type:\n",
            "  - Who â†’ personâ€™s name only (e.g., â€œGustave Eiffelâ€).\n",
            "  - When â†’ date/time only in the format used in Context (e.g., â€œJuly 20, 1969â€).\n",
            "  - Which/What â†’ the specific entity name or term (e.g., â€œThe Amazon Riverâ€).\n",
            "  - Where â†’ place name only.\n",
            "  - How many/How much â†’ the number as written in Context.\n",
            "- If multiple mentions exist, choose the most direct, specific, and shortest span that answers the Question.\n",
            "- Do not add trailing punctuation unless it is part of the answer span.\n",
            "- If the Context does not contain the answer, write: Unknown.\n",
            "\n",
            "Output format:\n",
            "Answer: <answer span only>\n",
            "2025-10-10 09:56:10,603 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:56:10,603 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task: Fill in the Answer line wi...'\n",
            "2025-10-10 09:56:10,605 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:56:10,605 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:56:12,408 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.80s.\n",
            "2025-10-10 09:56:12,410 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:56:12,411 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:56:12,412 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:56:12,412 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:56:15,357 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9500\n",
            "Reason: {\n",
            "  \"score\": 0.95,\n",
            "  \"reason\": \"The response is correct and includes the exact date. The only additional part is the prefix 'Answer:', which doesn't alter the correctness.\"\n",
            "}\n",
            "2025-10-10 09:56:15,359 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.3000\n",
            "Reason: {\n",
            "  \"score\": 0.3,\n",
            "  \"reason\": \"The response describes a characteristic of the Amazon River but does not provide its name explicitly.\"\n",
            "}\n",
            "2025-10-10 09:56:15,360 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, mentioning 'Gustave Eiffel' clearly.\"\n",
            "}\n",
            "2025-10-10 09:56:15,361 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:56:15,361 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.95s.\n",
            "2025-10-10 09:56:15,361 - fi.opt.optimizers.gepa - INFO - Scores: [0.95, 0.3, 1.0]\n",
            "2025-10-10 09:56:15,362 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 4.76s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [04:12<01:22,  9.17s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4: New subsample score 2.25 is not better than old score 2.3, skipping\n",
            "Iteration 5: Selected program 1 score: 0.96\n",
            "2025-10-10 09:56:15,364 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:56:15,365 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to ou...'\n",
            "2025-10-10 09:56:15,366 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:56:15,366 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:56:16,448 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.08s.\n",
            "2025-10-10 09:56:16,450 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:56:16,450 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:56:16,451 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:56:16,452 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:56:19,917 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, with only a difference in the case of the first letter, which is not significant.\"\n",
            "}\n",
            "2025-10-10 09:56:19,918 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response is nearly identical to the expected response, with only a minor omission of the definite article 'The'.\"\n",
            "}\n",
            "2025-10-10 09:56:19,920 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:56:19,922 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:56:19,923 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.47s.\n",
            "2025-10-10 09:56:19,923 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.9, 1.0]\n",
            "2025-10-10 09:56:19,924 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:56:19,924 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 4.56s.\n",
            "2025-10-10 09:56:19,925 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-10 09:56:19,926 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-10 09:56:19,927 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 5: Proposed new text for prompt: You will receive inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to output only the answer text to place after \"Answer:\". Follow these rules:\n",
            "\n",
            "- Use only information explicitly stated in the Context. Do not use outside knowledge or make inferences beyond the Context.\n",
            "- Return the shortest accurate phrase, value, or list that directly answers the Question.\n",
            "- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanation.\n",
            "- Reuse the exact wording from the Context whenever possible. If needed, minimally condense by removing hedges like â€œwidely regarded asâ€ while keeping the core claim intact.\n",
            "- Preserve the original capitalization, spelling, diacritics, hyphenation, numbers, units, and internal punctuation exactly as they appear in the Context. Do not change case (e.g., do not lowercase initial letters).\n",
            "- When copying a named entity or noun phrase, include any leading articles/determiners exactly as they appear in the Context (e.g., â€œThe Amazon River,â€ not â€œAmazon Riverâ€), preserving their capitalization.\n",
            "- Dates: copy exactly in the format used in the Context.\n",
            "- Lists: output just the items in the same order and wording as in the Context, preserving separators and conjunctions exactly as written (commas, â€œand,â€ â€œor,â€ em dashes, etc.).\n",
            "- â€œWhat is X known for?â€: return the primary accolade or superlative stated in the Context (prefer this over roles or examples). Itâ€™s acceptable to convert â€œis/was â€¦â€ into a minimal gerund phrase beginning with â€œBeing â€¦â€ to fit the question; if you introduce such a word, capitalize it appropriately.\n",
            "- If multiple phrasings exist in the Context, choose the most specific formulation that directly matches the Questionâ€™s wording (including determiners) and keep it verbatim.\n",
            "- Exclude any trailing sentence punctuation not intrinsic to the answer itself (e.g., omit a sentence-ending period).\n",
            "- If the Context does not contain the answer, output exactly: Not provided in context.\n",
            "- Output a single line with no leading or trailing spaces. Do not include any preamble, labels, or quotation marks.\n",
            "\n",
            "Procedure:\n",
            "1) Identify the minimal span in the Context that answers the Question.\n",
            "2) If the Question paraphrases a claim (e.g., â€œlargest by water volumeâ€ vs. â€œlargest by discharge volume of waterâ€), select the entity/value named in that claim from the Context.\n",
            "3) Copy the span verbatim, including determiners/articles and exact casing; only remove hedges if needed for brevity.\n",
            "4) For â€œknown forâ€ questions, prefer the core accolade/superlative; optionally render as â€œBeing â€¦â€ with proper capitalization.\n",
            "5) If no explicit answer is present, output: Not provided in context.\n",
            "2025-10-10 09:57:35,208 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:57:35,210 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will receive inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is t...'\n",
            "2025-10-10 09:57:35,211 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:57:35,211 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:57:38,563 - fi.opt.optimizers.gepa - INFO - Output generation finished in 3.35s.\n",
            "2025-10-10 09:57:38,565 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:57:38,566 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:57:38,567 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:57:38,567 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:57:42,098 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response captures the main idea of the expected response, focusing on being the greatest writer in the English language, although it omits the 'Being' at the beginning, which slightly affects completeness.\"\n",
            "}\n",
            "2025-10-10 09:57:42,100 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:57:42,101 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically identical to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:57:42,101 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:57:42,102 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.53s.\n",
            "2025-10-10 09:57:42,103 - fi.opt.optimizers.gepa - INFO - Scores: [0.9, 1.0, 1.0]\n",
            "2025-10-10 09:57:42,103 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.90s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "GEPA Optimization:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/40 [05:38<00:33, 11.02s/rollouts]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5: New subsample score 2.9 is not better than old score 2.9, skipping\n",
            "Iteration 6: Selected program 0 score: 0.7\n",
            "2025-10-10 09:57:42,105 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:57:42,106 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-10 09:57:42,106 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:57:42,107 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:57:44,081 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.97s.\n",
            "2025-10-10 09:57:44,083 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:57:44,084 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:57:44,084 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:57:44,084 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:57:48,039 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response accurately identifies the two products of photosynthesis, glucose and oxygen, just as the expected response. The inclusion of 'The products of photosynthesis are' is extraneous but does not affect the correctness semantically. A minor deduction is applied for unnecessary phrasing, hence the score of 0.9.\"\n",
            "}\n",
            "2025-10-10 09:57:48,042 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response correctly includes the date 'July 20, 1969', but adds extra information about a person walking on the Moon, which wasn't part of the expected response.\"\n",
            "}\n",
            "2025-10-10 09:57:48,044 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately includes all components of the expected response and maintains the order.\"\n",
            "}\n",
            "2025-10-10 09:57:48,044 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:57:48,045 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.96s.\n",
            "2025-10-10 09:57:48,046 - fi.opt.optimizers.gepa - INFO - Scores: [0.9, 0.8, 1.0]\n",
            "2025-10-10 09:57:48,047 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-10 09:57:48,050 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 5.94s.\n",
            "2025-10-10 09:57:48,050 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-10 09:57:48,050 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-10 09:57:48,051 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 6: Proposed new text for prompt: You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to produce the answer text only.\n",
            "\n",
            "Guidelines:\n",
            "- Use only the information stated in the provided context. Do not rely on outside knowledge.\n",
            "- Return the minimal text span that directly answers the question.\n",
            "- Output exactly and only the answer phrase. Do not include any extra words, explanations, or sentences (e.g., do not say â€œThe answer is â€¦â€ or restate the question).\n",
            "- Do not include the label â€œAnswer:â€ in your output.\n",
            "- Preserve the exact wording, capitalization, numerals, and formatting as they appear in the context (including dates and numbers). Do not reformat dates or numbers.\n",
            "- If the answer is a list, preserve the order and separators (e.g., commas, â€œandâ€) as in the context.\n",
            "- Do not add trailing punctuation unless it is part of the answer in the context (avoid adding a period by default).\n",
            "- Do not use quotation marks around the answer.\n",
            "- If the context does not provide the answer, output: Unknown\n",
            "\n",
            "Notes from examples (for consistency with expected grading):\n",
            "- Photosynthesis products should be given exactly as â€œGlucose and oxygenâ€ (not a full sentence).\n",
            "- The date of the first Moon walk should be given exactly as â€œJuly 20, 1969â€ (do not add extra wording).\n",
            "\n",
            "Example outputs:\n",
            "- Q: What are the products of photosynthesis? â†’ Glucose and oxygen\n",
            "- Q: When did a person first walk on the Moon? â†’ July 20, 1969\n",
            "2025-10-10 09:58:34,787 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:58:34,789 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to produce the answ...'\n",
            "2025-10-10 09:58:34,790 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-10 09:58:34,790 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:58:43,570 - fi.opt.optimizers.gepa - INFO - Output generation finished in 8.78s.\n",
            "2025-10-10 09:58:43,573 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:58:43,573 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:58:43,574 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-10 09:58:43,576 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:58:45,908 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:58:45,910 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is exactly the same as the expected response, indicating complete correctness.\"\n",
            "}\n",
            "2025-10-10 09:58:45,911 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:58:45,911 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-10 09:58:45,912 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.34s.\n",
            "2025-10-10 09:58:45,913 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 1.0]\n",
            "2025-10-10 09:58:45,913 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 11.13s.\n",
            "Iteration 6: New subsample score 3.0 is better than old score 2.7. Continue to full eval and add to candidate pool.\n",
            "2025-10-10 09:58:45,914 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-10 09:58:45,915 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to produce the answ...'\n",
            "2025-10-10 09:58:45,915 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-10 09:58:45,916 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-10 09:58:52,132 - fi.opt.optimizers.gepa - INFO - Output generation finished in 6.22s.\n",
            "2025-10-10 09:58:52,133 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-10 09:58:52,134 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-10 09:58:52,135 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-10 09:58:52,135 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-10 09:58:57,238 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is identical to the expected response, providing a perfect semantic match.\"\n",
            "}\n",
            "2025-10-10 09:58:57,240 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:58:57,241 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is identical to the expected response.\"\n",
            "}\n",
            "2025-10-10 09:58:57,242 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response 'Amazon River' is almost identical to the expected response 'The Amazon River', missing only the definite article.\"\n",
            "}\n",
            "2025-10-10 09:58:57,244 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response captures the main semantic content of the expected response, but lacks the introductory phrase 'Being'. It is almost equivalent but not fully identical.\"\n",
            "}\n",
            "2025-10-10 09:58:57,245 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-10 09:58:57,246 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.11s.\n",
            "2025-10-10 09:58:57,247 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 1.0, 0.9, 0.9]\n",
            "2025-10-10 09:58:57,247 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 11.33s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGEPA Optimization:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/40 [06:54<00:33, 11.19s/rollouts]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6: Full valset score for new program: 0.96\n",
            "Iteration 6: Full train_val score for new program: 0.96\n",
            "Iteration 6: Individual valset scores for new program: [1.0, 1.0, 1.0, 0.9, 0.9]\n",
            "Iteration 6: New valset pareto front scores: [1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "Iteration 6: Full valset pareto front score: 0.9800000000000001\n",
            "Iteration 6: Updated valset pareto front programs: [{1, 2}, {0, 2}, {1, 2}, {1, 2}, {1}]\n",
            "Iteration 6: Best valset aggregate score so far: 0.96\n",
            "Iteration 6: Best program as per aggregate score on train_val: 1\n",
            "Iteration 6: Best program as per aggregate score on valset: 1\n",
            "Iteration 6: Best score on valset: 0.96\n",
            "Iteration 6: Best score on train_val: 0.96\n",
            "Iteration 6: Linear pareto front program index: 1\n",
            "Iteration 6: New program candidate index: 2\n",
            "2025-10-10 09:58:57,250 - fi.opt.optimizers.gepa - INFO - gepa.optimize finished in 414.12s.\n",
            "2025-10-10 09:58:57,250 - fi.opt.optimizers.gepa - INFO - GEPA result best score: 0.96\n",
            "2025-10-10 09:58:57,251 - fi.opt.optimizers.gepa - INFO - GEPA best candidate: {'prompt': 'You are given inputs in the form:\\nContext: {context}\\nQuestion: {question}\\nAnswer:\\n\\nYour job is to output only the answer text, to be placed after \"Answer:\". Follow these rules:\\n\\n- Use only information from the provided Context. Do not use outside knowledge.\\n- Output the shortest accurate phrase, value, or list that directly answers the Question.\\n- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanation, examples, or extra context.\\n- Reuse exact wording from the Context when possible. If needed, minimally condense by removing hedges like â€œwidely regarded asâ€ to keep the core claim.\\n- Preserve the original capitalization, spelling, numbers, and units as they appear in the Context.\\n- If the answer is a date, output it exactly in the format given in the Context (e.g., \"July 20, 1969\").\\n- If the answer is a list, output just the items in the same order and wording as in the Context, joined as they appear (e.g., â€œglucose and oxygenâ€).\\n- For â€œWhat is X known for?â€ prefer the primary accolade or superlative stated in the Context (e.g., â€œbeing the greatest writer in the English languageâ€) over general roles or examples.\\n- Do not include any preamble, postscript, quotations, or punctuation beyond what is intrinsic to the answer itself. Output a single line with no leading or trailing spaces.\\n- If the Context does not contain the answer, output: Not provided in context.\\n\\nAim for maximal brevity while remaining fully accurate and faithful to the Context.'}\n",
            "2025-10-10 09:58:57,252 - fi.opt.optimizers.gepa - INFO - Translating GEPA result to OptimizationResult...\n",
            "2025-10-10 09:58:57,253 - fi.opt.optimizers.gepa - INFO - --- GEPA Prompt Optimization finished in 414.13s ---\n",
            "2025-10-10 09:58:57,253 - fi.opt.optimizers.gepa - INFO - Final best score: 0.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "results = optimizer.optimize(\n",
        "    evaluator = evaluator,\n",
        "    data_mapper = data_mapper,\n",
        "    dataset = dataset,\n",
        "    initial_prompts = [INITIAL_PROMPT],\n",
        "    max_metric_calls = 40 # Since our dataset is small and isn't too complex, a lower limit should suffice.\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d737702",
      "metadata": {
        "id": "1d737702"
      },
      "source": [
        "## Final Results of GEPA Optimizer Run\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Best Prompt Found\n",
        "print(f\"{results.best_generator.get_prompt_template()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dJLUazEYKMae",
        "outputId": "e7c3b956-180d-48f2-c4bc-3710f15345b4"
      },
      "id": "dJLUazEYKMae",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to output only the answer text, to be placed after \"Answer:\". Follow these rules:\n",
            "\n",
            "- Use only information from the provided Context. Do not use outside knowledge.\n",
            "- Output the shortest accurate phrase, value, or list that directly answers the Question.\n",
            "- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanation, examples, or extra context.\n",
            "- Reuse exact wording from the Context when possible. If needed, minimally condense by removing hedges like â€œwidely regarded asâ€ to keep the core claim.\n",
            "- Preserve the original capitalization, spelling, numbers, and units as they appear in the Context.\n",
            "- If the answer is a date, output it exactly in the format given in the Context (e.g., \"July 20, 1969\").\n",
            "- If the answer is a list, output just the items in the same order and wording as in the Context, joined as they appear (e.g., â€œglucose and oxygenâ€).\n",
            "- For â€œWhat is X known for?â€ prefer the primary accolade or superlative stated in the Context (e.g., â€œbeing the greatest writer in the English languageâ€) over general roles or examples.\n",
            "- Do not include any preamble, postscript, quotations, or punctuation beyond what is intrinsic to the answer itself. Output a single line with no leading or trailing spaces.\n",
            "- If the Context does not contain the answer, output: Not provided in context.\n",
            "\n",
            "Aim for maximal brevity while remaining fully accurate and faithful to the Context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Final Score\n",
        "results.final_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "fXM2knnxKyjD",
        "outputId": "ad6e4db8-a3d0-4590-b2e1-0adc73ea3934"
      },
      "id": "fXM2knnxKyjD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.96"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Iteration History\n",
        "for idx, hist in enumerate(results.history):\n",
        "  print(f\"---- Iteration {idx+1} ----\")\n",
        "  print(f\"===PROMPT===\\n{hist.prompt}\")\n",
        "  print(f\"\\n\\n===AVERAGE SCORE===\\n{hist.average_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "XreFo1r6LEiN",
        "outputId": "e2dec295-f61f-4d66-ab06-78caf42f1765"
      },
      "id": "XreFo1r6LEiN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Iteration 1 ----\n",
            "===PROMPT===\n",
            "Context: {context}\\nQuestion: {question}\\nAnswer:\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.7\n",
            "---- Iteration 2 ----\n",
            "===PROMPT===\n",
            "You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your job is to output only the answer text, to be placed after \"Answer:\". Follow these rules:\n",
            "\n",
            "- Use only information from the provided Context. Do not use outside knowledge.\n",
            "- Output the shortest accurate phrase, value, or list that directly answers the Question.\n",
            "- Prefer a concise noun phrase or value over a full sentence. Do not restate the question or add explanation, examples, or extra context.\n",
            "- Reuse exact wording from the Context when possible. If needed, minimally condense by removing hedges like â€œwidely regarded asâ€ to keep the core claim.\n",
            "- Preserve the original capitalization, spelling, numbers, and units as they appear in the Context.\n",
            "- If the answer is a date, output it exactly in the format given in the Context (e.g., \"July 20, 1969\").\n",
            "- If the answer is a list, output just the items in the same order and wording as in the Context, joined as they appear (e.g., â€œglucose and oxygenâ€).\n",
            "- For â€œWhat is X known for?â€ prefer the primary accolade or superlative stated in the Context (e.g., â€œbeing the greatest writer in the English languageâ€) over general roles or examples.\n",
            "- Do not include any preamble, postscript, quotations, or punctuation beyond what is intrinsic to the answer itself. Output a single line with no leading or trailing spaces.\n",
            "- If the Context does not contain the answer, output: Not provided in context.\n",
            "\n",
            "Aim for maximal brevity while remaining fully accurate and faithful to the Context.\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.96\n",
            "---- Iteration 3 ----\n",
            "===PROMPT===\n",
            "You will be given:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task is to produce the answer text only.\n",
            "\n",
            "Guidelines:\n",
            "- Use only the information stated in the provided context. Do not rely on outside knowledge.\n",
            "- Return the minimal text span that directly answers the question.\n",
            "- Output exactly and only the answer phrase. Do not include any extra words, explanations, or sentences (e.g., do not say â€œThe answer is â€¦â€ or restate the question).\n",
            "- Do not include the label â€œAnswer:â€ in your output.\n",
            "- Preserve the exact wording, capitalization, numerals, and formatting as they appear in the context (including dates and numbers). Do not reformat dates or numbers.\n",
            "- If the answer is a list, preserve the order and separators (e.g., commas, â€œandâ€) as in the context.\n",
            "- Do not add trailing punctuation unless it is part of the answer in the context (avoid adding a period by default).\n",
            "- Do not use quotation marks around the answer.\n",
            "- If the context does not provide the answer, output: Unknown\n",
            "\n",
            "Notes from examples (for consistency with expected grading):\n",
            "- Photosynthesis products should be given exactly as â€œGlucose and oxygenâ€ (not a full sentence).\n",
            "- The date of the first Moon walk should be given exactly as â€œJuly 20, 1969â€ (do not add extra wording).\n",
            "\n",
            "Example outputs:\n",
            "- Q: What are the products of photosynthesis? â†’ Glucose and oxygen\n",
            "- Q: When did a person first walk on the Moon? â†’ July 20, 1969\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33890fd1",
      "metadata": {
        "cellView": "form",
        "id": "33890fd1"
      },
      "outputs": [],
      "source": [
        "# @title Pick and choose from our range of Optimizers!!\n",
        "optimizer_name = \"PromptWizard\" # @param [\"PromptWizard\",\"RandomSearch\",\"ProTeGI\",\"MetaPrompt\",\"BayesianSearch\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6514aa61",
      "metadata": {
        "id": "6514aa61"
      },
      "source": [
        "---\n",
        "### Conclusion & Caveats\n",
        "\n",
        "- Different optimizers trade off cost, speed, and depth of edits.\n",
        "- **Meta-Prompt**, **ProTeGi**, and **GEPA** often find more robust prompts but cost more.\n",
        "- Use a local metric during development to reduce API costs, then run the teacher-guided optimizers for final refinement.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}